{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# fMRI Learning Stage Classification using Vision Transformers\n",
        "\n",
        "This notebook implements a Vision Transformer model to classify different stages of learning from fMRI data."
      ],
      "metadata": {
        "id": "RiSeSKEWOt6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract\n",
        "\n",
        "This research project presents an innovative approach to understanding the temporal dynamics of human learning through the analysis of functional Magnetic Resonance Imaging (fMRI) data using Vision Transformers (ViT). By leveraging advanced deep learning architectures, we aim to classify different stages of learning (early, middle, and late) based on neural activation patterns during a classification learning task.\n",
        "\n",
        "The study utilizes the \"[Classification Learning](https://openfmri.org/dataset/ds000002/)\", \"[Classification learning and tone-counting](https://openfmri.org/dataset/ds000011/)\", \"[Classification learning and stop-signal](https://openfmri.org/dataset/ds000017/)\", and \"[Classification learning and reversal](https://openfmri.org/dataset/ds000052/)\" datasets from OpenfMRI, which captures brain activity during a weather prediction task under both probabilistic and deterministic conditions. This dataset provides a unique opportunity to examine how the brain's activity patterns evolve as subjects progress through different learning phases, potentially revealing distinct neural signatures associated with each stage of skill acquisition."
      ],
      "metadata": {
        "id": "-ouChukWSjO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Understanding how the human brain adapts and reorganizes during learning remains a fundamental challenge in cognitive neuroscience. Traditional approaches to analyzing learning-related neural changes often rely on univariate analyses or conventional machine learning methods. However, these approaches may miss complex spatial and temporal patterns that characterize different learning stages.\n",
        "\n",
        "Our methodology introduces several key innovations:\n",
        "\n",
        "1. **Vision Transformer Architecture**: By adapting ViT models to process 3D fMRI data, we leverage the transformer's ability to capture long-range dependencies and complex spatial relationships within neural activation patterns. This approach treats brain volumes as sequences of patches, allowing the model to learn hierarchical representations of neural activity patterns.\n",
        "\n",
        "2. **Temporal Learning Classification**: The project aims to automatically identify and classify distinct phases of learning (early, middle, and late) based on whole-brain activation patterns. This classification could reveal how neural representations evolve throughout the learning process.\n",
        "\n",
        "3. **Multi-condition Analysis**: By incorporating both probabilistic and deterministic learning conditions, we can investigate how different types of learning rules affect neural activation patterns and their temporal evolution."
      ],
      "metadata": {
        "id": "KckweXWRSoQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected Outcomes and Impact\n",
        "\n",
        "This research has the potential to:\n",
        "\n",
        "- Identify neural markers of learning progression\n",
        "- Reveal differences in brain activation patterns between probabilistic and deterministic learning\n",
        "- Provide insights into individual differences in learning trajectories\n",
        "- Demonstrate the effectiveness of transformer-based architectures in neuroimaging analysis\n",
        "\n",
        "By successfully classifying learning stages from fMRI data, this work could contribute to our understanding of skill acquisition and learning optimization, with potential applications in educational neuroscience and cognitive rehabilitation."
      ],
      "metadata": {
        "id": "_lldQtzoS5HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies"
      ],
      "metadata": {
        "id": "bjYsMDqFg1oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install required packages"
      ],
      "metadata": {
        "id": "SqSBHENLg0O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops nibabel seaborn tqdm monai matplotlib nilearn plotly"
      ],
      "metadata": {
        "id": "pWoFBmlCg29l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import libraries"
      ],
      "metadata": {
        "id": "hGqWJ55Bg4Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from einops import rearrange, repeat\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "8112V4f6g6h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set random seeds"
      ],
      "metadata": {
        "id": "MlMUpXvzg9Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "sapR1Fl_g-eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Device configuration"
      ],
      "metadata": {
        "id": "XzumLODSiPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "7scVea8MiRON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### General configuration"
      ],
      "metadata": {
        "id": "mYmXm7q9jy7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    'patch_size': 8,\n",
        "    'hidden_dim': 512,      # Reduced from 768\n",
        "    'num_heads': 8,         # Reduced from 12\n",
        "    'num_layers': 6,        # Reduced from 12\n",
        "    'mlp_dim': 1024,        # Reduced from 3072\n",
        "    'dropout': 0.2,         # Increased from 0.1\n",
        "    'learning_rate': 5e-4,  # Increased from 1e-4\n",
        "    'weight_decay': 0.05,   # Increased from 0.01\n",
        "    'batch_size': 16,       # Increased from 8\n",
        "    'epochs': 50,\n",
        "    'warmup_steps': 50\n",
        "}"
      ],
      "metadata": {
        "id": "82UbY3F-j2ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using device: {device}\")\n",
        "print(\"\\nConfiguration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "mw02n1gUj4Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "C5Zl0AOihM8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download and extract dataset"
      ],
      "metadata": {
        "id": "gkrgjf4vhTdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)"
      ],
      "metadata": {
        "id": "FgGCaCcXeq9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                           miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)"
      ],
      "metadata": {
        "id": "6yWcYT4RlUYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_extracted(extract_path, dataset_id):\n",
        "    \"\"\"Check if dataset is properly extracted by looking for expected structure\"\"\"\n",
        "    extract_path = Path(extract_path)\n",
        "\n",
        "    # Check common indicators of proper extraction\n",
        "    indicators = {\n",
        "        'ds000002': ['sub-01', 'sub-02', 'dataset_description.json'],\n",
        "        'ds000011': ['sub-01', 'sub-02', 'dataset_description.json'],\n",
        "        'ds000017': ['sub-01', 'sub-02', 'dataset_description.json'],\n",
        "        'ds000052': ['sub-01', 'sub-02', 'dataset_description.json']\n",
        "    }\n",
        "\n",
        "    # First check if the path exists\n",
        "    if not extract_path.exists():\n",
        "        return False\n",
        "\n",
        "    # Look for dataset in subfolders if not found in main directory\n",
        "    possible_roots = [extract_path] + list(extract_path.glob('*'))\n",
        "\n",
        "    for root in possible_roots:\n",
        "        if not root.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Check for expected files/folders\n",
        "        found_indicators = []\n",
        "        for indicator in indicators[dataset_id]:\n",
        "            if any(Path(p).name == indicator for p in root.glob('*')):\n",
        "                found_indicators.append(indicator)\n",
        "\n",
        "        # If we found at least 2 indicators, consider it properly extracted\n",
        "        if len(found_indicators) >= 2:\n",
        "            print(f\"Found valid dataset structure in: {root}\")\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "a5Nzf4kHf1zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_datasets(base_path):\n",
        "    \"\"\"Download and extract multiple fMRI datasets\"\"\"\n",
        "    # Create base directory if it doesn't exist\n",
        "    base_path = Path(base_path)\n",
        "    base_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Dataset information\n",
        "    datasets = {\n",
        "        'ds000002': {\n",
        "            'url': 'https://s3.amazonaws.com/openneuro/ds000002/ds000002_R2.0.5/compressed/ds000002_R2.0.5_raw.zip',\n",
        "            'filename': 'ds000002_R2.0.5_raw.zip',\n",
        "            'extract_dir': 'ds000002'\n",
        "        },\n",
        "        'ds000011': {\n",
        "            'url': 'https://s3.amazonaws.com/openneuro/ds000011/ds000011_R2.0.1/compressed/ds000011_R2.0.1_raw.zip',\n",
        "            'filename': 'ds000011_R2.0.1_raw.zip',\n",
        "            'extract_dir': 'ds000011'\n",
        "        },\n",
        "        'ds000017': {\n",
        "            'url': 'https://s3.amazonaws.com/openneuro/ds000017/ds000017_R2.0.1/compressed/ds000017_R2.0.1.zip',\n",
        "            'filename': 'ds000017_R2.0.1.zip',\n",
        "            'extract_dir': 'ds000017'\n",
        "        },\n",
        "        'ds000052': {\n",
        "            'url': 'https://s3.amazonaws.com/openneuro/ds000052/ds000052_R2.0.0/compressed/ds052_R2.0.0_01-14.tgz',\n",
        "            'filename': 'ds052_R2.0.0_01-14.tgz',\n",
        "            'extract_dir': 'ds000052'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Process each dataset\n",
        "    dataset_paths = {}\n",
        "    for dataset_id, info in datasets.items():\n",
        "        print(f\"\\nProcessing {dataset_id}...\")\n",
        "\n",
        "        # Setup paths\n",
        "        zip_path = base_path / info['filename']\n",
        "        extract_path = base_path / 'fmri_data' / info['extract_dir']\n",
        "        dataset_paths[dataset_id] = extract_path\n",
        "\n",
        "        # Download if needed\n",
        "        if not zip_path.exists():\n",
        "            print(f\"Downloading {dataset_id}...\")\n",
        "            try:\n",
        "                download_url(info['url'], zip_path)\n",
        "                print(\"Download complete!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {dataset_id}: {str(e)}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Found existing download for {dataset_id}\")\n",
        "\n",
        "        # Extract if needed\n",
        "        if not extract_path.exists():\n",
        "            print(f\"Extracting {dataset_id}...\")\n",
        "            try:\n",
        "                extract_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                if zip_path.suffix == '.tgz':\n",
        "                    with tarfile.open(zip_path, 'r:gz') as tar_ref:\n",
        "                        tar_ref.extractall(extract_path)\n",
        "                else:\n",
        "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                        zip_ref.extractall(extract_path)\n",
        "\n",
        "                print(\"Extraction complete!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting {dataset_id}: {str(e)}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Found existing extracted data for {dataset_id}\")\n",
        "\n",
        "    return dataset_paths"
      ],
      "metadata": {
        "id": "BkB3Z0yylVxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive"
      ],
      "metadata": {
        "id": "nJcURekoe6uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UNXJ4cpje5uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup base path"
      ],
      "metadata": {
        "id": "GXtMlX4he8pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = Path('/content/drive/MyDrive/learnedSpectrum')"
      ],
      "metadata": {
        "id": "l3G-kA-Qe-X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Download and extract all datasets"
      ],
      "metadata": {
        "id": "6NCtVt9cfADh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = setup_datasets(base_path)"
      ],
      "metadata": {
        "id": "osz3fyQDfBbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataset locations:\")\n",
        "for dataset_id, path in dataset_paths.items():\n",
        "    print(f\"{dataset_id}: {path}\")"
      ],
      "metadata": {
        "id": "ZCmp88OffC93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FMRI Volume Loader"
      ],
      "metadata": {
        "id": "fmSnNfKzj7f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fmri_volume(file_path):\n",
        "    \"\"\"Load and preprocess a single fMRI volume\"\"\"\n",
        "    # Load nifti file\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "\n",
        "    # Handle 4D data (take middle 20% of timepoints)\n",
        "    if len(data.shape) == 4:\n",
        "        mid = data.shape[-1] // 2\n",
        "        window = data.shape[-1] // 10\n",
        "        data = data[..., mid-window:mid+window]\n",
        "        data = np.mean(data, axis=-1)\n",
        "\n",
        "    # Normalize\n",
        "    data = (data - np.percentile(data, 5)) / (np.percentile(data, 95) - np.percentile(data, 5) + 1e-8)\n",
        "\n",
        "    # Basic brain extraction\n",
        "    mask = data > np.percentile(data, 20)\n",
        "    data = data * mask\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "I6H7r-0gj90V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FMRI loader test:\")\n",
        "test_path = list(Path(dataset_paths['ds000002']).rglob('*bold.nii.gz'))[0]\n",
        "test_data = load_fmri_volume(test_path)\n",
        "print(f\"Loaded volume shape: {test_data.shape}\")"
      ],
      "metadata": {
        "id": "CNZ5_JTXj--D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Volume Patchification"
      ],
      "metadata": {
        "id": "5kPOP9jFkAjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_patches(volume, patch_size):\n",
        "    \"\"\"Convert 3D volume into patches with proper batch dimension\"\"\"\n",
        "    # Ensure volume dimensions are divisible by patch_size\n",
        "    pad_h = (patch_size - volume.shape[0] % patch_size) % patch_size\n",
        "    pad_w = (patch_size - volume.shape[1] % patch_size) % patch_size\n",
        "    pad_d = (patch_size - volume.shape[2] % patch_size) % patch_size\n",
        "\n",
        "    # Pad volume\n",
        "    volume = np.pad(volume,\n",
        "                   ((0, pad_h), (0, pad_w), (0, pad_d)),\n",
        "                   mode='constant')\n",
        "\n",
        "    # Create patches using einops\n",
        "    patches = rearrange(volume,\n",
        "                       '(h p1) (w p2) (d p3) -> (h w d) (p1 p2 p3)',\n",
        "                       p1=patch_size, p2=patch_size, p3=patch_size)\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "JUCPfVu1kCmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_patches = create_patches(test_data, CONFIG['patch_size'])\n",
        "print(\"\\nPatch creation test:\")\n",
        "print(f\"Input shape: {test_data.shape}\")\n",
        "print(f\"Output patches shape: {test_patches.shape}\")"
      ],
      "metadata": {
        "id": "traZBAUFkEVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Creation"
      ],
      "metadata": {
        "id": "HDYi40btkFPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_task_files(dataset_path):\n",
        "    \"\"\"Get all task-related fMRI files with their stages\"\"\"\n",
        "    files = []\n",
        "    for bold_file in Path(dataset_path).rglob('*bold.nii.gz'):\n",
        "        # Skip if not a task file\n",
        "        if 'task-' not in str(bold_file):\n",
        "            continue\n",
        "\n",
        "        # Get run number\n",
        "        run_match = re.search(r'run-(\\d+)', str(bold_file))\n",
        "        if not run_match:\n",
        "            continue\n",
        "        run_num = int(run_match.group(1))\n",
        "\n",
        "        # Assign early/late stage\n",
        "        if run_num == 1:\n",
        "            stage = 0  # early\n",
        "        elif run_num > 1:\n",
        "            stage = 1  # late\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        files.append((bold_file, stage))\n",
        "\n",
        "    return files"
      ],
      "metadata": {
        "id": "3_OkLlc1kHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = get_task_files(dataset_paths['ds000002'])\n",
        "print(\"\\nFile collection test:\")\n",
        "print(f\"Found {len(test_files)} task files\")\n",
        "print(\"Sample entries:\")\n",
        "for f, s in test_files[:3]:\n",
        "    print(f\"File: {f.name}, Stage: {s}\")"
      ],
      "metadata": {
        "id": "Ot2oh8sykI4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer Components"
      ],
      "metadata": {
        "id": "cbzRrl9ZkSkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Position Embedding"
      ],
      "metadata": {
        "id": "S3Ysw7HZkTyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_position_embeddings(n_patches, hidden_dim):\n",
        "    \"\"\"Create learnable position embeddings\"\"\"\n",
        "    pos_embeddings = nn.Parameter(torch.randn(1, n_patches + 1, hidden_dim))\n",
        "    return pos_embeddings"
      ],
      "metadata": {
        "id": "Z_wvrgUxkVyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CLS Token"
      ],
      "metadata": {
        "id": "ATRMlV52kYbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cls_token(hidden_dim):\n",
        "    \"\"\"Create learnable classification token\"\"\"\n",
        "    cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "    return cls_token"
      ],
      "metadata": {
        "id": "TNV7WU2HkaSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_n_patches = test_patches.shape[0]  # from previous section\n",
        "pos_embed = create_position_embeddings(test_n_patches, CONFIG['hidden_dim'])\n",
        "cls_token = create_cls_token(CONFIG['hidden_dim'])\n",
        "print(f\"Position embedding shape: {pos_embed.shape}\")\n",
        "print(f\"CLS token shape: {cls_token.shape}\")"
      ],
      "metadata": {
        "id": "CLuhFbzckbcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Head Attention Block"
      ],
      "metadata": {
        "id": "L-8owAcekdGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(hidden_dim, hidden_dim * 3)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Apply attention to V\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JUXZXjwNkffI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = MultiHeadAttention(CONFIG['hidden_dim'], CONFIG['num_heads']).to(device)\n",
        "test_input = torch.randn(2, test_n_patches + 1, CONFIG['hidden_dim']).to(device)\n",
        "test_output = attention(test_input)\n",
        "print(f\"\\nAttention test:\")\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {test_output.shape}\")"
      ],
      "metadata": {
        "id": "SsMcr8V0khJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Block"
      ],
      "metadata": {
        "id": "t88PDcfykjNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, mlp_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "f4BS4WMtkklt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPBlock(CONFIG['hidden_dim'], CONFIG['mlp_dim']).to(device)\n",
        "test_output = mlp(test_input)\n",
        "print(f\"\\nMLP test:\")\n",
        "print(f\"Output shape: {test_output.shape}\")"
      ],
      "metadata": {
        "id": "WjhxUBm_knCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Encoder Block"
      ],
      "metadata": {
        "id": "wZiKp6goko_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.attn = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "JRTNZNQ5kq5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TransformerBlock(\n",
        "    CONFIG['hidden_dim'],\n",
        "    CONFIG['num_heads'],\n",
        "    CONFIG['mlp_dim']\n",
        ").to(device)\n",
        "test_output = transformer(test_input)\n",
        "print(f\"\\nTransformer block test:\")\n",
        "print(f\"Output shape: {test_output.shape}\")"
      ],
      "metadata": {
        "id": "eBJQk8CVksqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Patch Embedding"
      ],
      "metadata": {
        "id": "WLQeifRjkuiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(patch_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # Ensure input is 3D [B, N, D]\n",
        "        if len(patches.shape) == 2:\n",
        "            patches = patches.unsqueeze(0)\n",
        "\n",
        "        return self.projection(patches)"
      ],
      "metadata": {
        "id": "dL2f5MJHkwZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_dim = CONFIG['patch_size'] ** 3  # cubic patches\n",
        "patch_embed = PatchEmbedding(patch_dim, CONFIG['hidden_dim']).to(device)\n",
        "test_patches_tensor = torch.FloatTensor(test_patches).to(device)\n",
        "embedded_patches = patch_embed(test_patches_tensor)\n",
        "print(f\"\\nPatch embedding test:\")\n",
        "print(f\"Input shape: {test_patches_tensor.shape}\")\n",
        "print(f\"Embedded shape: {embedded_patches.shape}\")"
      ],
      "metadata": {
        "id": "nfurBcH7kxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_n_patches(volume_shape, patch_size):\n",
        "    \"\"\"Calculate number of patches based on input volume shape\"\"\"\n",
        "    h, w, d = volume_shape\n",
        "\n",
        "    # Add padding if needed\n",
        "    h = h + (patch_size - h % patch_size) % patch_size\n",
        "    w = w + (patch_size - w % patch_size) % patch_size\n",
        "    d = d + (patch_size - d % patch_size) % patch_size\n",
        "\n",
        "    # Calculate number of patches\n",
        "    n_patches = (h // patch_size) * (w // patch_size) * (d // patch_size)\n",
        "    return n_patches"
      ],
      "metadata": {
        "id": "oXS6APTcoEby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complete Vision Transformer"
      ],
      "metadata": {
        "id": "-j1UPOWxkyzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, volume_shape=(64, 64, 32)):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        patch_dim = config['patch_size'] ** 3\n",
        "\n",
        "        # Calculate number of patches\n",
        "        self.n_patches = calculate_n_patches(volume_shape, config['patch_size'])\n",
        "        print(f\"Number of patches: {self.n_patches}\")\n",
        "\n",
        "        # Layers\n",
        "        self.patch_embed = PatchEmbedding(patch_dim, config['hidden_dim'])\n",
        "        self.cls_token = create_cls_token(config['hidden_dim'])\n",
        "        self.pos_embed = create_position_embeddings(\n",
        "            self.n_patches,\n",
        "            config['hidden_dim']\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                config['hidden_dim'],\n",
        "                config['num_heads'],\n",
        "                config['mlp_dim'],\n",
        "                config['dropout']\n",
        "            )\n",
        "            for _ in range(config['num_layers'])\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(config['hidden_dim'])\n",
        "        self.head = nn.Linear(config['hidden_dim'], 2)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # Ensure correct input shape\n",
        "        if len(patches.shape) == 2:  # [N, D]\n",
        "            patches = patches.unsqueeze(0)  # Add batch dimension [1, N, D]\n",
        "        elif len(patches.shape) == 3 and patches.shape[1] == 1:  # [B, 1, N*D]\n",
        "            patches = patches.squeeze(1)  # Remove singleton dimension\n",
        "\n",
        "        B = patches.shape[0]  # Batch size\n",
        "\n",
        "        # Embed patches\n",
        "        x = self.patch_embed(patches)  # [B, N, hidden_dim]\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=B)  # [B, 1, hidden_dim]\n",
        "        x = torch.cat([cls_tokens, x], dim=1)  # [B, N+1, hidden_dim]\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Classification\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # Take CLS token\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fD5XXlPik1cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_patches = create_patches(test_data, CONFIG['patch_size'])\n",
        "test_patches_tensor = torch.FloatTensor(test_patches).to(device)\n",
        "print(f\"Input shape (single): {test_patches_tensor.shape}\")"
      ],
      "metadata": {
        "id": "OEHtzXRok2_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(CONFIG, volume_shape=(64, 64, 32)).to(device)\n",
        "test_output = model(test_patches_tensor)\n",
        "print(f\"Output shape (single): {test_output.shape}\")"
      ],
      "metadata": {
        "id": "VUsSw4WDn8VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "test_batch = test_patches_tensor.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "print(f\"Input shape (batch): {test_batch.shape}\")\n",
        "test_output = model(test_batch)\n",
        "print(f\"Output shape (batch): {test_output.shape}\")"
      ],
      "metadata": {
        "id": "e-05S3CoqcoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Training Pipeline"
      ],
      "metadata": {
        "id": "3GmXjV97k9BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FMRI Dataset Class"
      ],
      "metadata": {
        "id": "xTep7764k-hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_volume_to_size(volume, target_size=(64, 64, 32)):\n",
        "    \"\"\"Pad volume to target size\"\"\"\n",
        "    pad_h = max(0, target_size[0] - volume.shape[0])\n",
        "    pad_w = max(0, target_size[1] - volume.shape[1])\n",
        "    pad_d = max(0, target_size[2] - volume.shape[2])\n",
        "\n",
        "    padded = np.pad(volume,\n",
        "                    ((0, pad_h), (0, pad_w), (0, pad_d)),\n",
        "                    mode='constant')\n",
        "\n",
        "    # If larger than target size, crop\n",
        "    padded = padded[:target_size[0], :target_size[1], :target_size[2]]\n",
        "\n",
        "    return padded"
      ],
      "metadata": {
        "id": "QUuPdfkUov87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FMRIDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, patch_size=8, augment=False, target_size=(64, 64, 32)):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.patch_size = patch_size\n",
        "        self.augment = augment\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def get_class_weights(labels):\n",
        "        counts = np.bincount(labels)\n",
        "        total = len(labels)\n",
        "        weights = total / (len(counts) * counts)\n",
        "        weights = torch.FloatTensor(weights)\n",
        "        print(f\"Class weights: {weights}\")\n",
        "        return weights\n",
        "\n",
        "    # Update data augmentation\n",
        "    def apply_augmentation(volume):\n",
        "        \"\"\"More aggressive augmentation\"\"\"\n",
        "        # Random flip\n",
        "        if np.random.random() > 0.5:\n",
        "            volume = np.flip(volume, axis=0)\n",
        "        if np.random.random() > 0.5:\n",
        "            volume = np.flip(volume, axis=1)\n",
        "        if np.random.random() > 0.5:\n",
        "            volume = np.flip(volume, axis=2)\n",
        "\n",
        "        # Random rotation with interpolation\n",
        "        angle = np.random.uniform(-15, 15)\n",
        "        volume = scipy.ndimage.rotate(volume, angle, axes=(0,1), reshape=False)\n",
        "\n",
        "        # Random scaling\n",
        "        scale = np.random.uniform(0.8, 1.2)\n",
        "        volume = volume * scale\n",
        "\n",
        "        # Add random noise\n",
        "        noise = np.random.normal(0, 0.05, volume.shape)\n",
        "        volume = volume + noise\n",
        "\n",
        "        # Random intensity shift\n",
        "        shift = np.random.uniform(-0.1, 0.1)\n",
        "        volume = volume + shift\n",
        "\n",
        "        # Random contrast\n",
        "        contrast = np.random.uniform(0.8, 1.2)\n",
        "        mean = volume.mean()\n",
        "        volume = (volume - mean) * contrast + mean\n",
        "\n",
        "        return volume\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load and preprocess volume\n",
        "        volume = load_fmri_volume(self.file_paths[idx])\n",
        "        volume = pad_volume_to_size(volume, self.target_size)\n",
        "\n",
        "        if self.augment:\n",
        "            volume = self.apply_augmentation(volume)\n",
        "\n",
        "        # Create patches\n",
        "        patches = create_patches(volume, self.patch_size)\n",
        "        patches = torch.FloatTensor(patches)\n",
        "\n",
        "        return patches, torch.tensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "KVGHzb9_k-MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle batching properly\"\"\"\n",
        "    patches = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    # Stack patches and labels\n",
        "    patches = torch.stack(patches)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return patches, labels"
      ],
      "metadata": {
        "id": "1WNWNzNTp6st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Collection"
      ],
      "metadata": {
        "id": "jEmdxCxslDqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_dataset_files():\n",
        "    all_files = []\n",
        "    all_labels = []\n",
        "\n",
        "    for dataset_id, path in dataset_paths.items():\n",
        "        print(f\"\\nProcessing {dataset_id}...\")\n",
        "        files = get_task_files(path)\n",
        "        print(f\"Found {len(files)} files\")\n",
        "\n",
        "        for file_path, label in files:\n",
        "            all_files.append(file_path)\n",
        "            all_labels.append(label)\n",
        "\n",
        "    return all_files, all_labels"
      ],
      "metadata": {
        "id": "nPh_HQDUlFhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files, labels = collect_dataset_files()\n",
        "print(f\"\\nTotal samples: {len(files)}\")\n",
        "print(f\"Class distribution: {np.bincount(labels)}\")"
      ],
      "metadata": {
        "id": "OpU5xN32lHkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train-Val Split"
      ],
      "metadata": {
        "id": "ZbII1F7KlIr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_splits(files, labels, config):\n",
        "    # Create train/val split\n",
        "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "        files, labels,\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Target size for all volumes\n",
        "    target_size = (64, 64, 32)  # This ensures consistent size\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FMRIDataset(\n",
        "        train_files,\n",
        "        train_labels,\n",
        "        patch_size=config['patch_size'],\n",
        "        augment=True,\n",
        "        target_size=target_size\n",
        "    )\n",
        "\n",
        "    val_dataset = FMRIDataset(\n",
        "        val_files,\n",
        "        val_labels,\n",
        "        patch_size=config['patch_size'],\n",
        "        augment=False,\n",
        "        target_size=target_size\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "wJWW9ZrJlQXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = create_data_splits(files, labels, CONFIG)\n",
        "print(f\"\\nTrain samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "Nnji2EJVlRju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Components"
      ],
      "metadata": {
        "id": "zvzGwlNblS2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(train_dataset, val_dataset, batch_size):\n",
        "    # Use fewer workers and persistent workers\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Run in main process\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn,\n",
        "        persistent_workers=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Run in main process\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn,\n",
        "        persistent_workers=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "QH47gDeQlUZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimizer(model, config):\n",
        "    # Split parameters into two groups for different learning rates\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters()\n",
        "                      if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': config['weight_decay']\n",
        "        },\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters()\n",
        "                      if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=config['learning_rate'],\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "HVzJECuiy2FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_scheduler(optimizer, config, num_training_steps):\n",
        "    return get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=config['warmup_steps'],\n",
        "        num_training_steps=num_training_steps,\n",
        "        num_cycles=0.5\n",
        "    )"
      ],
      "metadata": {
        "id": "PcfAY87ty3wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(labels):\n",
        "    counts = np.bincount(labels)\n",
        "    total = len(labels)\n",
        "    weights = total / (len(counts) * counts)\n",
        "    weights = torch.FloatTensor(weights)\n",
        "    print(f\"Class weights: {weights}\")\n",
        "    return weights"
      ],
      "metadata": {
        "id": "GEEoYtpNBo2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = get_class_weights(labels)\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights.to(device),\n",
        "    label_smoothing=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "iXAbbLT4lW9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders"
      ],
      "metadata": {
        "id": "B1cUSJtDla_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = create_dataloaders(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    CONFIG['batch_size']\n",
        ")"
      ],
      "metadata": {
        "id": "XfHHUsaWlYS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Functions"
      ],
      "metadata": {
        "id": "IdDN9iUDlgWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for patches, labels in pbar:\n",
        "        patches = patches.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use mixed precision training\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(patches)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/(pbar.n+1):.4f}',\n",
        "            'acc': f'{100.*correct/total:.2f}%',\n",
        "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
        "        })\n",
        "\n",
        "    return total_loss / len(train_loader), correct / total"
      ],
      "metadata": {
        "id": "UmlXmM2Flj3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for patches, labels in tqdm(val_loader, desc='Validation'):\n",
        "        patches = patches.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(patches)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader), correct / total"
      ],
      "metadata": {
        "id": "M65cNCiVlmCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop and Visualization"
      ],
      "metadata": {
        "id": "WMcgCJlilr9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training History Tracker"
      ],
      "metadata": {
        "id": "MsZkWzmhl3rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingHistory:\n",
        "    def __init__(self):\n",
        "        self.train_loss = []\n",
        "        self.train_acc = []\n",
        "        self.val_loss = []\n",
        "        self.val_acc = []\n",
        "        self.best_acc = 0\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def update(self, train_metrics, val_metrics, epoch):\n",
        "        train_loss, train_acc = train_metrics\n",
        "        val_loss, val_acc = val_metrics\n",
        "\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.train_acc.append(train_acc)\n",
        "        self.val_loss.append(val_loss)\n",
        "        self.val_acc.append(val_acc)\n",
        "\n",
        "        if val_acc > self.best_acc:\n",
        "            self.best_acc = val_acc\n",
        "            self.best_epoch = epoch\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "history = TrainingHistory()"
      ],
      "metadata": {
        "id": "78SsIe9Hl5LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization Functions"
      ],
      "metadata": {
        "id": "r-ziLjlUl6nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history.train_loss, label='Train')\n",
        "    ax1.plot(history.val_loss, label='Validation')\n",
        "    ax1.set_title('Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history.train_acc, label='Train')\n",
        "    ax2.plot(history.val_acc, label='Validation')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2K8v_lLhl8ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(true_labels, pred_labels):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Early', 'Late'],\n",
        "                yticklabels=['Early', 'Late'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9F3sFfZ1l9mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop"
      ],
      "metadata": {
        "id": "-tu7Hx6Ql_IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n",
        "    history = TrainingHistory()\n",
        "    best_val_acc = 0\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            # Use tqdm with leave=True to keep progress bar\n",
        "            train_pbar = tqdm(train_loader, desc='Training', leave=True)\n",
        "            for patches, labels in train_pbar:\n",
        "                # Move to device\n",
        "                patches = patches.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(patches)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update metrics\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                train_pbar.set_postfix({\n",
        "                    'loss': f'{train_loss/train_total:.4f}',\n",
        "                    'acc': f'{100.*train_correct/train_total:.2f}%'\n",
        "                })\n",
        "\n",
        "            # Calculate epoch metrics\n",
        "            train_loss = train_loss / len(train_loader)\n",
        "            train_acc = train_correct / train_total\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_pbar = tqdm(val_loader, desc='Validation', leave=True)\n",
        "                for patches, labels in val_pbar:\n",
        "                    patches = patches.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(patches)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    val_pbar.set_postfix({\n",
        "                        'loss': f'{val_loss/val_total:.4f}',\n",
        "                        'acc': f'{100.*val_correct/val_total:.2f}%'\n",
        "                    })\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            val_loss = val_loss / len(val_loader)\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            # Update learning rate\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            # Update history\n",
        "            is_best = history.update((train_loss, train_acc), (val_loss, val_acc), epoch)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\nEpoch Summary:\")\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "            print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if is_best:\n",
        "                print(\"New best model saved!\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'val_acc': val_acc,\n",
        "                }, 'best_model.pth')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # Early stopping check\n",
        "            if patience_counter >= patience:\n",
        "                print(\"\\nEarly stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted by user!\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "x0myodcbmBDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Evaluation"
      ],
      "metadata": {
        "id": "6Gwe_DHdmCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for patches, labels in tqdm(dataloader, desc='Evaluating'):\n",
        "        patches = patches.to(device)\n",
        "        outputs = model(patches)\n",
        "        _, preds = outputs.max(1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)"
      ],
      "metadata": {
        "id": "K4O--HKcmEKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Debug"
      ],
      "metadata": {
        "id": "7ittw0LcrnHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting data pipeline:\")\n",
        "test_batch = next(iter(train_loader))\n",
        "patches, labels = test_batch\n",
        "print(f\"Batch patches shape: {patches.shape}\")\n",
        "print(f\"Batch labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "jirsCx3RqDzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nPatches statistics:\")\n",
        "print(f\"Min value: {patches.min().item():.4f}\")\n",
        "print(f\"Max value: {patches.max().item():.4f}\")\n",
        "print(f\"Mean value: {patches.mean().item():.4f}\")\n",
        "print(f\"Std value: {patches.std().item():.4f}\")"
      ],
      "metadata": {
        "id": "td3BvQzMrkqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels, counts = labels.unique(return_counts=True)\n",
        "print(\"\\nLabel distribution in batch:\")\n",
        "for label, count in zip(unique_labels.tolist(), counts.tolist()):\n",
        "    print(f\"Label {label}: {count}\")"
      ],
      "metadata": {
        "id": "HKDl8VTFrmA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting forward pass:\")\n",
        "model = VisionTransformer(CONFIG, volume_shape=(64, 64, 32)).to(device)\n",
        "optimizer = create_optimizer(model, CONFIG)\n",
        "num_training_steps = len(train_loader) * CONFIG['epochs']\n",
        "scheduler = create_scheduler(optimizer, CONFIG, num_training_steps)\n",
        "test_output = model(patches.to(device))\n",
        "print(f\"Model output shape: {test_output.shape}\")"
      ],
      "metadata": {
        "id": "mn65s-NfqE0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Training"
      ],
      "metadata": {
        "id": "v0s2PvHumFPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    epochs=CONFIG['epochs']\n",
        ")"
      ],
      "metadata": {
        "id": "RDlskZgbmG8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training history"
      ],
      "metadata": {
        "id": "wz2wGe5bmIgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "xDEGjLzdmJgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load best model and evaluate"
      ],
      "metadata": {
        "id": "tPJ3P63OmKmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating best model...\")\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "pred_labels, true_labels = evaluate_model(model, val_loader)"
      ],
      "metadata": {
        "id": "Oe2gxLulmLf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot confusion matrix"
      ],
      "metadata": {
        "id": "VHF7hWZ6mMxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(true_labels, pred_labels)"
      ],
      "metadata": {
        "id": "7JWYcz2vmO0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print final metrics"
      ],
      "metadata": {
        "id": "YYwUaDxlmP2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, pred_labels,\n",
        "                          target_names=['Early Stage', 'Late Stage']))"
      ],
      "metadata": {
        "id": "kxYdYHhZmQ_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results"
      ],
      "metadata": {
        "id": "LvegYpNLmWMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(history, model_path='best_model.pth'):\n",
        "    results = {\n",
        "        'config': CONFIG,\n",
        "        'history': {\n",
        "            'train_loss': history.train_loss,\n",
        "            'train_acc': history.train_acc,\n",
        "            'val_loss': history.val_loss,\n",
        "            'val_acc': history.val_acc,\n",
        "            'best_acc': history.best_acc,\n",
        "            'best_epoch': history.best_epoch\n",
        "        },\n",
        "        'model_path': model_path\n",
        "    }\n",
        "\n",
        "    np.save('training_results.npy', results)\n",
        "    print(\"\\nResults saved!\")"
      ],
      "metadata": {
        "id": "7IeuXQ-AmX_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results(history)"
      ],
      "metadata": {
        "id": "4h_mwUurmZlz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}