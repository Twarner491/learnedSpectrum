{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# fMRI Learning Stage Classification using Vision Transformers\n",
        "\n",
        "This notebook implements a Vision Transformer (ViT) model to classify different stages of learning from fMRI data.\n",
        "\n",
        "The dataset used is the \"Classification learning\" dataset from OpenfMRI."
      ],
      "metadata": {
        "id": "RiSeSKEWOt6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies"
      ],
      "metadata": {
        "id": "omHV0EvAOxpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops nibabel seaborn tqdm"
      ],
      "metadata": {
        "id": "gHtOBbWGO0qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from einops import rearrange, repeat\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "q3O4NRU7O2ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and Download Dataset"
      ],
      "metadata": {
        "id": "9r109NQXO_Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Mount Google Drive"
      ],
      "metadata": {
        "id": "UbfG_BpwO4Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zk0ixsm-O5tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up paths"
      ],
      "metadata": {
        "id": "JdwE0lt_O6x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/learnedSpectrum'\n",
        "zip_path = os.path.join(base_path, \"ds000002_R2.0.5_raw.zip\")\n",
        "extract_path = os.path.join(base_path, \"fmri_data\")"
      ],
      "metadata": {
        "id": "goW5gHMzO8Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset if not already present"
      ],
      "metadata": {
        "id": "hfecSKHcPBOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    url = \"https://s3.amazonaws.com/openneuro/ds000002/ds000002_R2.0.5/compressed/ds000002_R2.0.5_raw.zip\"\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(\"Download complete!\")"
      ],
      "metadata": {
        "id": "p45FKs13PCyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract dataset if not already extracted"
      ],
      "metadata": {
        "id": "ZJjk0McaPECn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(extract_path):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete!\")"
      ],
      "metadata": {
        "id": "Loupj67kPFGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "jVn_lGgmqnkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedFMRIAugmentation:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Ensure input is tensor\n",
        "        x = torch.as_tensor(x)\n",
        "\n",
        "        # More aggressive augmentation for small dataset\n",
        "        if torch.rand(1) < self.p:\n",
        "            # Gaussian noise with random intensity\n",
        "            noise_level = torch.rand(1) * 0.1\n",
        "            x = x + torch.randn_like(x) * noise_level\n",
        "\n",
        "            # Random intensity scaling\n",
        "            scale = 0.8 + torch.rand(1) * 0.4  # Scale between 0.8 and 1.2\n",
        "            x = x * scale\n",
        "\n",
        "            # Random rotations (90 degree increments)\n",
        "            if torch.rand(1) < 0.5:\n",
        "                x = torch.rot90(x, k=torch.randint(1, 4, (1,)).item(), dims=(1, 2))\n",
        "\n",
        "            # Random flips\n",
        "            if torch.rand(1) < 0.5:\n",
        "                x = torch.flip(x, dims=[torch.randint(1, 4, (1,)).item()])\n",
        "\n",
        "            # Elastic deformation (subtle)\n",
        "            if torch.rand(1) < 0.3:\n",
        "                sigma = torch.rand(1) * 3\n",
        "                x = self._elastic_transform(x, sigma=sigma.item())\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _elastic_transform(self, x, sigma=3):\n",
        "        shape = x.shape\n",
        "        dx = torch.randn(*shape) * sigma\n",
        "        dy = torch.randn(*shape) * sigma\n",
        "        dz = torch.randn(*shape) * sigma\n",
        "\n",
        "        x_new = x + dx\n",
        "        y_new = x + dy\n",
        "        z_new = x + dz\n",
        "\n",
        "        return (x_new + y_new + z_new) / 3.0"
      ],
      "metadata": {
        "id": "_G39wEZZqtBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionPooling(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_dim // 2, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights = self.attention(x)\n",
        "        weighted = torch.sum(weights * x, dim=1)\n",
        "        return weighted"
      ],
      "metadata": {
        "id": "nc6P6DyDkXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "BN8UO1meqv-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedfMRIDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, phase='train'):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "        self.samples = []\n",
        "        self.temporal_window = 8\n",
        "        self._load_samples()\n",
        "\n",
        "    def _find_dataset_root(self):\n",
        "        \"\"\"Recursively find the directory containing subject folders\"\"\"\n",
        "        print(f\"\\nSearching for dataset root starting from: {self.root_dir}\")\n",
        "\n",
        "        for root, dirs, _ in os.walk(self.root_dir):\n",
        "            if 'ds002_R2.0.5' in dirs:\n",
        "                ds_path = os.path.join(root, 'ds002_R2.0.5')\n",
        "                sub_dirs = [d for d in os.listdir(ds_path) if d.startswith('sub-')]\n",
        "                if sub_dirs:\n",
        "                    print(f\"Found subject directories in: {ds_path}\")\n",
        "                    return ds_path, sorted(sub_dirs)\n",
        "\n",
        "        return None, []\n",
        "\n",
        "    def _normalize_volume(self, volume):\n",
        "        \"\"\"Robust normalization for fMRI data\"\"\"\n",
        "        if volume.ndim == 4:\n",
        "            # Normalize each timepoint independently\n",
        "            for t in range(volume.shape[-1]):\n",
        "                vol_t = volume[..., t]\n",
        "                mask = vol_t != 0\n",
        "                if mask.any():\n",
        "                    p1, p99 = np.percentile(vol_t[mask], (1, 99))\n",
        "                    volume[..., t] = np.clip((vol_t - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
        "\n",
        "                    # Z-score normalization within mask\n",
        "                    mean = np.mean(volume[..., t][mask])\n",
        "                    std = np.std(volume[..., t][mask])\n",
        "                    volume[..., t][mask] = (volume[..., t][mask] - mean) / (std + 1e-8)\n",
        "        else:\n",
        "            # Single volume normalization\n",
        "            mask = volume != 0\n",
        "            if mask.any():\n",
        "                p1, p99 = np.percentile(volume[mask], (1, 99))\n",
        "                volume = np.clip((volume - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
        "\n",
        "                # Z-score normalization within mask\n",
        "                mean = np.mean(volume[mask])\n",
        "                std = np.std(volume[mask])\n",
        "                volume[mask] = (volume[mask] - mean) / (std + 1e-8)\n",
        "\n",
        "        return volume\n",
        "\n",
        "    def _pad_or_crop(self, volume, target_shape):\n",
        "        \"\"\"Center-aligned padding or cropping\"\"\"\n",
        "        if volume.ndim == 4:\n",
        "            result = np.zeros((*target_shape, volume.shape[-1]), dtype=np.float32)\n",
        "        else:\n",
        "            result = np.zeros(target_shape, dtype=np.float32)\n",
        "\n",
        "        for i in range(3):\n",
        "            if volume.shape[i] > target_shape[i]:\n",
        "                # Center crop\n",
        "                start = (volume.shape[i] - target_shape[i]) // 2\n",
        "                end = start + target_shape[i]\n",
        "                slices = [slice(None)] * volume.ndim\n",
        "                slices[i] = slice(start, end)\n",
        "                volume = volume[tuple(slices)]\n",
        "            else:\n",
        "                # Center pad\n",
        "                pad_before = (target_shape[i] - volume.shape[i]) // 2\n",
        "                pad_after = target_shape[i] - volume.shape[i] - pad_before\n",
        "                pad_width = [(0, 0)] * volume.ndim\n",
        "                pad_width[i] = (pad_before, pad_after)\n",
        "                volume = np.pad(volume, pad_width, mode='constant')\n",
        "\n",
        "        return volume\n",
        "\n",
        "    def _preprocess_volume(self, img_data):\n",
        "        \"\"\"Enhanced preprocessing pipeline with proper temporal handling\"\"\"\n",
        "        # Handle 4D data with temporal sampling\n",
        "        if len(img_data.shape) == 4:\n",
        "            # Select evenly spaced timepoints\n",
        "            total_timepoints = img_data.shape[-1]\n",
        "            indices = np.linspace(0, total_timepoints-1, self.temporal_window, dtype=int)\n",
        "            img_data = img_data[..., indices]\n",
        "\n",
        "            # Ensure consistent spatial dimensions\n",
        "            target_size = (64, 64, 64)\n",
        "            temp_data = np.zeros((*target_size, len(indices)), dtype=np.float32)\n",
        "\n",
        "            # Process each timepoint\n",
        "            for t in range(len(indices)):\n",
        "                vol = self._pad_or_crop(img_data[..., t], target_size)\n",
        "                temp_data[..., t] = vol\n",
        "\n",
        "            img_data = temp_data\n",
        "        else:\n",
        "            # Single volume case\n",
        "            target_size = (64, 64, 64)\n",
        "            img_data = self._pad_or_crop(img_data, target_size)\n",
        "\n",
        "        # Normalize\n",
        "        img_data = self._normalize_volume(img_data)\n",
        "\n",
        "        return img_data.astype(np.float32)\n",
        "\n",
        "    def _load_samples(self):\n",
        "        print(f\"\\nLoading {self.phase} samples...\")\n",
        "\n",
        "        # Find the actual dataset root directory\n",
        "        dataset_root, subjects = self._find_dataset_root()\n",
        "        if not dataset_root:\n",
        "            raise ValueError(f\"Could not find subject directories in {self.root_dir}\")\n",
        "\n",
        "        print(f\"Dataset root: {dataset_root}\")\n",
        "        print(f\"Found {len(subjects)} subjects: {subjects}\")\n",
        "\n",
        "        all_samples = []\n",
        "\n",
        "        # First, collect all valid samples\n",
        "        for subject in subjects:\n",
        "            func_path = os.path.join(dataset_root, subject, 'func')\n",
        "            print(f\"\\nChecking subject {subject} func path: {func_path}\")\n",
        "\n",
        "            if not os.path.exists(func_path):\n",
        "                print(f\"No func directory found for subject {subject}\")\n",
        "                continue\n",
        "\n",
        "            # Collect files by task\n",
        "            subject_files = {}\n",
        "            for file in os.listdir(func_path):\n",
        "                if file.endswith('_bold.nii.gz') and 'task-' in file and 'mixedeventrelatedprobe' not in file:\n",
        "                    task = file.split('task-')[1].split('_')[0]\n",
        "                    if task not in subject_files:\n",
        "                        subject_files[task] = []\n",
        "                    subject_files[task].append(file)\n",
        "\n",
        "            print(f\"Found files by task: {subject_files.keys()}\")\n",
        "\n",
        "            subject_samples = []\n",
        "            for task, files in subject_files.items():\n",
        "                sorted_files = sorted(files)\n",
        "                if len(sorted_files) >= 2:\n",
        "                    # First run is early stage\n",
        "                    early_file = sorted_files[0]\n",
        "                    file_path = os.path.join(func_path, early_file)\n",
        "                    subject_samples.append((file_path, 0))\n",
        "                    print(f\"Added early stage sample: {early_file}\")\n",
        "\n",
        "                    # Last run is late stage\n",
        "                    late_file = sorted_files[-1]\n",
        "                    file_path = os.path.join(func_path, late_file)\n",
        "                    subject_samples.append((file_path, 1))\n",
        "                    print(f\"Added late stage sample: {late_file}\")\n",
        "\n",
        "            if len(subject_samples) >= 2:\n",
        "                all_samples.extend(subject_samples)\n",
        "                print(f\"Added {len(subject_samples)} samples from subject {subject}\")\n",
        "            else:\n",
        "                print(f\"Skipped subject {subject} - insufficient samples\")\n",
        "\n",
        "        print(f\"\\nTotal collected samples: {len(all_samples)}\")\n",
        "\n",
        "        if len(all_samples) == 0:\n",
        "            raise ValueError(\"No valid samples found in the dataset!\")\n",
        "\n",
        "        # Sort for consistent splits\n",
        "        all_samples.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Split samples while maintaining class balance\n",
        "        early_samples = [s for s in all_samples if s[1] == 0]\n",
        "        late_samples = [s for s in all_samples if s[1] == 1]\n",
        "\n",
        "        print(f\"\\nTotal early samples: {len(early_samples)}\")\n",
        "        print(f\"Total late samples: {len(late_samples)}\")\n",
        "\n",
        "        # Calculate split indices based on phase\n",
        "        if self.phase == 'train':\n",
        "            early_split = early_samples[:int(len(early_samples) * 0.7)]\n",
        "            late_split = late_samples[:int(len(late_samples) * 0.7)]\n",
        "        elif self.phase == 'val':\n",
        "            early_split = early_samples[int(len(early_samples) * 0.7):int(len(early_samples) * 0.85)]\n",
        "            late_split = late_samples[int(len(late_samples) * 0.7):int(len(late_samples) * 0.85)]\n",
        "        else:  # test\n",
        "            early_split = early_samples[int(len(early_samples) * 0.85):]\n",
        "            late_split = late_samples[int(len(late_samples) * 0.85):]\n",
        "\n",
        "        self.samples = early_split + late_split\n",
        "\n",
        "        # Print final class distribution\n",
        "        early_count = sum(1 for s in self.samples if s[1] == 0)\n",
        "        late_count = sum(1 for s in self.samples if s[1] == 1)\n",
        "\n",
        "        print(f\"\\nFinal class distribution in {self.phase} set:\")\n",
        "        print(f\"Early stage (0): {early_count} samples\")\n",
        "        print(f\"Late stage (1): {late_count} samples\")\n",
        "        print(f\"Total: {len(self.samples)} samples\")\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for {self.phase} set after splitting!\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            file_path, stage = self.samples[idx]\n",
        "            img = nib.load(file_path)\n",
        "            img_data = img.get_fdata()\n",
        "\n",
        "            # Preprocess\n",
        "            img_data = self._preprocess_volume(img_data)\n",
        "\n",
        "            # Convert to tensor\n",
        "            img_tensor = torch.from_numpy(img_data).float()\n",
        "\n",
        "            # Handle dimensionality\n",
        "            if len(img_tensor.shape) == 4:  # If we have temporal dimension\n",
        "                # [H, W, D, T] -> [T, H, W, D] -> [1, H, W, D]\n",
        "                img_tensor = img_tensor.permute(3, 0, 1, 2)\n",
        "                # Average across temporal dimension\n",
        "                img_tensor = img_tensor.mean(dim=0, keepdim=True)\n",
        "            else:\n",
        "                # Add channel dimension for 3D volume\n",
        "                img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "            if self.transform and self.phase == 'train':\n",
        "                img_tensor = self.transform(img_tensor)\n",
        "\n",
        "            return img_tensor, torch.tensor(stage, dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            # Return a properly shaped tensor in case of error\n",
        "            return torch.zeros((1, 64, 64, 64)), torch.tensor(-1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "fUMr63Y0q29e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "z9b6yNUmq6fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Patch Embedding"
      ],
      "metadata": {
        "id": "67OorgtMs0JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed3D(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=8, in_channels=1, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_size = (img_size, img_size, img_size)\n",
        "        self.patch_size = (patch_size, patch_size, patch_size)\n",
        "        self.n_patches = (img_size // patch_size) ** 3\n",
        "\n",
        "        # Single projection layer\n",
        "        self.proj = nn.Conv3d(in_channels, embed_dim,\n",
        "                             kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Add LayerNorm\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W, D = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = rearrange(x, 'b e h w d -> b (h w d) e')\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "h6eZ8eJIq9QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Mechanism"
      ],
      "metadata": {
        "id": "QNHOUduxs2T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s5QIRa1Hs3VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Block"
      ],
      "metadata": {
        "id": "CDfHAqf6s4pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlockWithResidual(nn.Module):\n",
        "    \"\"\"Transformer block with residual attention and stochastic depth\"\"\"\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=2., dropout=0.1, drop_path=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, dropout=dropout)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "0JL7H8D0s566"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Vision Transformer Model"
      ],
      "metadata": {
        "id": "4yOmgoeotI20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedVisionTransformer3D(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=8, in_channels=1, num_classes=2,\n",
        "                 embed_dim=128, depth=4, num_heads=4, mlp_ratio=2., dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Reduced complexity for small dataset\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, embed_dim//2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LayerNorm([embed_dim//2, img_size//2, img_size//2, img_size//2]),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv3d(embed_dim//2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LayerNorm([embed_dim, img_size//4, img_size//4, img_size//4]),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        num_patches = (img_size // 4) ** 3\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Transformer blocks with residual attention\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlockWithResidual(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Improved classification head with self-attention pooling\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.attention_pool = SelfAttentionPooling(embed_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim//2),\n",
        "            nn.LayerNorm(embed_dim//2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim//2, num_classes)\n",
        "        )\n",
        "\n",
        "        # Weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Forward pass with feature extraction option\n",
        "        x = self.patch_embed(x)\n",
        "        x = rearrange(x, 'b c h w d -> b (h w d) c')\n",
        "\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + self.pos_embed[:, :x.size(1)]\n",
        "\n",
        "        features = []\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            features.append(x[:, 0])  # Store CLS token features\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.attention_pool(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        return logits"
      ],
      "metadata": {
        "id": "KzuApXUptL_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_curriculum(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    scaler = GradScaler()\n",
        "    best_val_acc = 0.0\n",
        "    curriculum_phases = [\n",
        "        {'epoch': 0, 'dropout': 0.4, 'aug_prob': 0.7},\n",
        "        {'epoch': 5, 'dropout': 0.3, 'aug_prob': 0.5},\n",
        "        {'epoch': 10, 'dropout': 0.2, 'aug_prob': 0.3}\n",
        "    ]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Update curriculum phase\n",
        "        current_phase = next(phase for phase in reversed(curriculum_phases)\n",
        "                           if epoch >= phase['epoch'])\n",
        "\n",
        "        # Update model dropout\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Dropout):\n",
        "                module.p = current_phase['dropout']\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                outputs, features = model(inputs, return_features=True)\n",
        "                # Main classification loss\n",
        "                cls_loss = criterion(outputs, labels)\n",
        "\n",
        "                # Feature consistency loss\n",
        "                consistency_loss = feature_consistency_loss(features)\n",
        "\n",
        "                # Total loss\n",
        "                loss = cls_loss + 0.1 * consistency_loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
        "              f'Train Acc: {100.*train_correct/total:.2f}%')\n",
        "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
        "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
        "\n",
        "        # Save best model\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "y9tfvJ6jkrif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_consistency_loss(features):\n",
        "    \"\"\"Calculate consistency loss between features from different layers\"\"\"\n",
        "    loss = 0\n",
        "    for i in range(len(features)-1):\n",
        "        loss += F.mse_loss(F.normalize(features[i]), F.normalize(features[i+1]))\n",
        "    return loss / (len(features)-1)"
      ],
      "metadata": {
        "id": "gU6xY61NkuSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "7LGskAMjq9oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    'img_size': 64,\n",
        "    'patch_size': 8,\n",
        "    'in_channels': 1,\n",
        "    'embed_dim': 128,\n",
        "    'depth': 4,\n",
        "    'num_heads': 4,\n",
        "    'mlp_ratio': 2.,\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.2\n",
        "}"
      ],
      "metadata": {
        "id": "TiIs2d7vk68f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_config = {\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "    'batch_size': 4,\n",
        "    'num_epochs': 30\n",
        "}"
      ],
      "metadata": {
        "id": "TZ8kdc1KllMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_training():\n",
        "    # Initialize datasets with improved augmentation\n",
        "    augmentation = AdvancedFMRIAugmentation(p=0.5)\n",
        "    train_dataset = ImprovedfMRIDataset(root_dir=extract_path, transform=augmentation, phase='train')\n",
        "    val_dataset = ImprovedfMRIDataset(root_dir=extract_path, transform=None, phase='val')\n",
        "\n",
        "    # Get class weights for balanced training\n",
        "    labels = torch.tensor([sample[1] for sample in train_dataset.samples])\n",
        "    all_classes = np.array([0, 1])\n",
        "\n",
        "    class_counts = np.bincount(labels.numpy(), minlength=2)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=all_classes,\n",
        "        y=labels.numpy()\n",
        "    )\n",
        "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "    # Create data loaders with smaller batch size\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,  # Reduced batch size\n",
        "        sampler=WeightedRandomSampler(\n",
        "            weights=[class_weights[label] for label in labels],\n",
        "            num_samples=len(labels),\n",
        "            replacement=True\n",
        "        ),\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=2,  # Reduced batch size\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model with smaller architecture\n",
        "    model = ImprovedVisionTransformer3D(\n",
        "        img_size=64,\n",
        "        patch_size=8,\n",
        "        in_channels=1,\n",
        "        num_classes=2,\n",
        "        embed_dim=128,  # Reduced embedding dimension\n",
        "        depth=6,        # Reduced number of transformer blocks\n",
        "        num_heads=4,    # Reduced number of attention heads\n",
        "        mlp_ratio=2.,   # Reduced MLP ratio\n",
        "        qkv_bias=True,\n",
        "        drop_rate=0.3,\n",
        "        attn_drop_rate=0.2\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss function with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(\n",
        "        weight=class_weights,\n",
        "        label_smoothing=0.1\n",
        "    )\n",
        "\n",
        "    # Optimizer with weight decay\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=1e-4,\n",
        "        weight_decay=0.05,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=1e-4,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    return model, criterion, optimizer, scheduler, train_loader, val_loader"
      ],
      "metadata": {
        "id": "X5Vq-Vw6rD-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "0W6mtjYqrFJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    # Enable gradient scaler for mixed precision training\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    best_val_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    # Enable automatic memory management\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_bar):\n",
        "            try:\n",
        "                # Move to GPU and convert to half precision\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "\n",
        "                if scaler is not None:\n",
        "                    with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Update metrics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Update progress bar\n",
        "                train_bar.set_postfix({\n",
        "                    'batch': f'{batch_idx+1}/{len(train_loader)}',\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{100.*train_correct/train_total:.2f}%'\n",
        "                })\n",
        "\n",
        "                # Clear GPU cache periodically\n",
        "                if batch_idx % 5 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training batch {batch_idx}: {str(e)}\")\n",
        "                torch.cuda.empty_cache()  # Clear cache on error\n",
        "                continue\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc='Validation')\n",
        "            for inputs, labels in val_bar:\n",
        "                try:\n",
        "                    inputs = inputs.to(device, non_blocking=True)\n",
        "                    labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                    with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    val_bar.set_postfix({\n",
        "                        'loss': f'{loss.item():.4f}',\n",
        "                        'acc': f'{100.*val_correct/val_total:.2f}%'\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in validation batch: {str(e)}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    continue\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Save best model (use CPU for saving)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model_state,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Clear cache after each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "bSdQ53ATrItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution"
      ],
      "metadata": {
        "id": "4DiqABZ0rNpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device"
      ],
      "metadata": {
        "id": "IXiqD_aVrQ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "oyNpRZX4rR4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set random seeds for reproducibility"
      ],
      "metadata": {
        "id": "H_Z8850JrT9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "XG0JaiX8rU7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup training components"
      ],
      "metadata": {
        "id": "_mS29CkqrY3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ImprovedVisionTransformer3D(**model_config).to(device)"
      ],
      "metadata": {
        "id": "HQxOmm2Jram9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print model summary"
      ],
      "metadata": {
        "id": "ex3C46Lmrcqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel Parameters:\")\n",
        "print(f\"Total: {total_params:,}\")\n",
        "print(f\"Trainable: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "ixOqy4KprdqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training configuration"
      ],
      "metadata": {
        "id": "bmOi7q-trge8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 18\n",
        "early_stopping_patience = 15\n",
        "early_stopping_counter = 0\n",
        "best_val_acc = 0.0"
      ],
      "metadata": {
        "id": "kduQN4Lqrhzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "VznUdoB1rk8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_with_curriculum(model, train_loader, val_loader,\n",
        "                            criterion, optimizer, scheduler,\n",
        "                            config['num_epochs'], device)"
      ],
      "metadata": {
        "id": "f2peeYy6rl9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training history"
      ],
      "metadata": {
        "id": "yEd4-ByTrrNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Acc')\n",
        "plt.plot(history['val_acc'], label='Val Acc')\n",
        "plt.title('Accuracy History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PiQMgFltrsiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load best model for evaluation"
      ],
      "metadata": {
        "id": "rLG2heWMr2mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading best model for evaluation...\")\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cYxTPUBXr37w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare test dataset and loader"
      ],
      "metadata": {
        "id": "FzrbUx-vr59-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on test set"
      ],
      "metadata": {
        "id": "9w8VSWO-r_iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating model on test set...\")\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "all_preds = []\n",
        "all_labels = []"
      ],
      "metadata": {
        "id": "019RyCULsCAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\nEvaluating model on test set...\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Get unique classes actually present in the data\n",
        "    unique_classes = np.unique(np.concatenate([all_labels, all_preds]))\n",
        "    class_names = ['Early', 'Late']\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for label in range(2):  # We expect binary classification\n",
        "        count = np.sum(all_labels == label)\n",
        "        print(f\"Class {class_names[label]}: {count} samples\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Compute and print detailed metrics for available classes\n",
        "    print(\"\\nDetailed Metrics:\")\n",
        "    try:\n",
        "        # Try to compute classification report with present classes\n",
        "        report = classification_report(\n",
        "            all_labels,\n",
        "            all_preds,\n",
        "            labels=range(len(unique_classes)),\n",
        "            target_names=[class_names[i] for i in unique_classes],\n",
        "            digits=4,\n",
        "            zero_division=0\n",
        "        )\n",
        "        print(report)\n",
        "\n",
        "        # Additional metrics\n",
        "        metrics = {\n",
        "            'Accuracy': test_acc / 100,\n",
        "            'Precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "            'Recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "            'F1-Score': f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        }\n",
        "\n",
        "        print(\"\\nAggregated Metrics:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not compute some metrics: {str(e)}\")\n",
        "        metrics = {'Accuracy': test_acc / 100}\n",
        "\n",
        "    # Return results\n",
        "    results = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss / len(test_loader),\n",
        "        'predictions': all_preds,\n",
        "        'true_labels': all_labels,\n",
        "        'confusion_matrix': cm,\n",
        "        'metrics': metrics\n",
        "    }\n",
        "\n",
        "    # Save results to file\n",
        "    np.save('test_results.npy', results)\n",
        "    print(\"\\nResults saved to 'test_results.npy'\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "qAS2JFbzsGHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading best model for evaluation...\")\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "SVQ6OsMrZcKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_dataset():\n",
        "    # Create test dataset with balanced classes\n",
        "    test_dataset = ImprovedfMRIDataset(root_dir=extract_path, transform=None, phase='test')\n",
        "\n",
        "    # Print class distribution before testing\n",
        "    labels = [sample[1] for sample in test_dataset.samples]\n",
        "    print(\"\\nTest Set Class Distribution:\")\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    for label, count in zip(unique_labels, counts):\n",
        "        print(f\"Class {'Early' if label == 0 else 'Late'}: {count} samples\")\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "# Create and print test dataset distribution\n",
        "test_dataset = prepare_test_dataset()\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "RQgyNyuvZgP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "id": "ntghiqnOZh2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save final results"
      ],
      "metadata": {
        "id": "HSlFMCKEse8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'history': history,\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_predictions': all_preds,\n",
        "    'test_labels': all_labels,\n",
        "    'best_val_accuracy': checkpoint['best_val_acc']\n",
        "}"
      ],
      "metadata": {
        "id": "noiQu484sgpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'results': results,\n",
        "    'training_args': {\n",
        "        'num_epochs': num_epochs,\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "        'weight_decay': 0.05,\n",
        "        'architecture': str(model)\n",
        "    }\n",
        "}, 'final_model_and_results.pth')"
      ],
      "metadata": {
        "id": "Pj_xjQxSskut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining complete! Model and results saved.\")\n",
        "print(f\"Best validation accuracy: {checkpoint['best_val_acc']:.2f}%\")\n",
        "print(f\"Final test accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "xt9LFObssqyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}