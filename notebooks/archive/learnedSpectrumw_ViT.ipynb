{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# fMRI Learning Stage Classification using Vision Transformers\n",
        "\n",
        "This notebook implements a Vision Transformer (ViT) model to classify different stages of learning from fMRI data.\n",
        "\n",
        "The dataset used is the \"Classification learning\" dataset from OpenfMRI."
      ],
      "metadata": {
        "id": "RiSeSKEWOt6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies"
      ],
      "metadata": {
        "id": "omHV0EvAOxpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops nibabel seaborn tqdm"
      ],
      "metadata": {
        "id": "gHtOBbWGO0qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from einops import rearrange\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "q3O4NRU7O2ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and Download Dataset"
      ],
      "metadata": {
        "id": "9r109NQXO_Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Mount Google Drive"
      ],
      "metadata": {
        "id": "UbfG_BpwO4Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zk0ixsm-O5tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up paths"
      ],
      "metadata": {
        "id": "JdwE0lt_O6x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/learnedSpectrum'\n",
        "zip_path = os.path.join(base_path, \"ds000002_R2.0.5_raw.zip\")\n",
        "extract_path = os.path.join(base_path, \"fmri_data\")"
      ],
      "metadata": {
        "id": "goW5gHMzO8Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset if not already present"
      ],
      "metadata": {
        "id": "hfecSKHcPBOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    url = \"https://s3.amazonaws.com/openneuro/ds000002/ds000002_R2.0.5/compressed/ds000002_R2.0.5_raw.zip\"\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(\"Download complete!\")"
      ],
      "metadata": {
        "id": "p45FKs13PCyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract dataset if not already extracted"
      ],
      "metadata": {
        "id": "ZJjk0McaPECn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(extract_path):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete!\")"
      ],
      "metadata": {
        "id": "Loupj67kPFGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Components"
      ],
      "metadata": {
        "id": "xGD0RItMPHKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D Patch Embedding"
      ],
      "metadata": {
        "id": "Vu8KsIymPIjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed3D(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=8, in_channels=1, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_size = (img_size, img_size, img_size)\n",
        "        self.patch_size = (patch_size, patch_size, patch_size)\n",
        "        self.n_patches = (img_size // patch_size) ** 3\n",
        "\n",
        "        # Single projection layer\n",
        "        self.proj = nn.Conv3d(in_channels, embed_dim,\n",
        "                             kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Add LayerNorm\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W, D = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = rearrange(x, 'b e h w d -> b (h w d) e')\n",
        "        x = self.norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jtaxtdCsPKpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mechanism"
      ],
      "metadata": {
        "id": "G0vXgF8WPMSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l8C6wDWQPNi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block"
      ],
      "metadata": {
        "id": "sJez3ry3PO3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(int(dim * mlp_ratio), dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_VG4nmbZPQS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Vision Transformer Model"
      ],
      "metadata": {
        "id": "12YdtsTuPRoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedVisionTransformer3D(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=8, in_channels=1, num_classes=2,\n",
        "                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4., qkv_bias=True,\n",
        "                 drop_rate=0.1, attn_drop_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Single patch embedding with fixed dimension\n",
        "        self.patch_embed = PatchEmbed3D(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        num_patches = self.patch_embed.n_patches\n",
        "\n",
        "        # Classification token and position embedding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # Dropout and layer norm\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        self.norm_pre = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.pre_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=drop_rate),\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=drop_rate)\n",
        "        )\n",
        "        self.head = nn.Linear(embed_dim // 2, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "        print(f\"Model initialized with:\")\n",
        "        print(f\"- Patch size: {patch_size}x{patch_size}x{patch_size}\")\n",
        "        print(f\"- Number of patches: {num_patches}\")\n",
        "        print(f\"- Embedding dimension: {embed_dim}\")\n",
        "        print(f\"- Number of transformer blocks: {depth}\")\n",
        "        print(f\"- Number of attention heads: {num_heads}\")\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "        self.apply(self._init_fn)\n",
        "\n",
        "    def _init_fn(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add classification token\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embedding and dropout\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        x = self.norm_pre(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # Take cls token only\n",
        "        x = self.pre_head(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wfhw6o_jPWYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Implementation"
      ],
      "metadata": {
        "id": "MUy3hAVXPX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fMRIDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = os.path.join(root_dir, 'ds002_R2.0.5')\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        print(\"Loading samples...\")\n",
        "        for subject in os.listdir(self.root_dir):\n",
        "            if not subject.startswith('sub-'):\n",
        "                continue\n",
        "\n",
        "            func_path = os.path.join(self.root_dir, subject, 'func')\n",
        "            if not os.path.exists(func_path):\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(func_path):\n",
        "                if not file.endswith('_bold.nii.gz'):\n",
        "                    continue\n",
        "\n",
        "                file_path = os.path.join(func_path, file)\n",
        "                task = file.split('task-')[1].split('_')[0]\n",
        "                run = int(file.split('run-')[1].split('_')[0])\n",
        "                stage = 0 if run == 1 else 1\n",
        "\n",
        "                self.samples.append((file_path, stage, task))\n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "\n",
        "    def _pad_or_crop(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Pad or crop a 3D volume to target shape.\n",
        "\n",
        "        Args:\n",
        "            volume (np.ndarray): Input volume of shape (H, W, D)\n",
        "            target_shape (tuple): Desired output shape\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Processed volume of shape target_shape\n",
        "        \"\"\"\n",
        "        result = np.zeros(target_shape, dtype=np.float32)\n",
        "\n",
        "        # Calculate padding/cropping for each dimension\n",
        "        pad_width = []\n",
        "        slices = []\n",
        "        for i in range(3):\n",
        "            if volume.shape[i] > target_shape[i]:\n",
        "                # Need to crop\n",
        "                start = (volume.shape[i] - target_shape[i]) // 2\n",
        "                end = start + target_shape[i]\n",
        "                slices.append(slice(start, end))\n",
        "            else:\n",
        "                # Need to pad\n",
        "                pad_before = (target_shape[i] - volume.shape[i]) // 2\n",
        "                pad_after = target_shape[i] - volume.shape[i] - pad_before\n",
        "                pad_width.append((pad_before, pad_after))\n",
        "                slices.append(slice(None))\n",
        "\n",
        "        # Handle cropping first\n",
        "        if volume.shape[0] > target_shape[0]:\n",
        "            volume = volume[slices[0], :, :]\n",
        "        if volume.shape[1] > target_shape[1]:\n",
        "            volume = volume[:, slices[1], :]\n",
        "        if volume.shape[2] > target_shape[2]:\n",
        "            volume = volume[:, :, slices[2]]\n",
        "\n",
        "        # Handle padding\n",
        "        if volume.shape[0] < target_shape[0]:\n",
        "            pad_before = (target_shape[0] - volume.shape[0]) // 2\n",
        "            result[pad_before:pad_before+volume.shape[0], :volume.shape[1], :volume.shape[2]] = volume\n",
        "        else:\n",
        "            result[:, :volume.shape[1], :volume.shape[2]] = volume\n",
        "\n",
        "        if volume.shape[1] < target_shape[1]:\n",
        "            pad_before = (target_shape[1] - volume.shape[1]) // 2\n",
        "            temp = np.zeros_like(result)\n",
        "            temp[:, pad_before:pad_before+volume.shape[1], :] = result[:, :volume.shape[1], :]\n",
        "            result = temp\n",
        "\n",
        "        if volume.shape[2] < target_shape[2]:\n",
        "            pad_before = (target_shape[2] - volume.shape[2]) // 2\n",
        "            temp = np.zeros_like(result)\n",
        "            temp[:, :, pad_before:pad_before+volume.shape[2]] = result[:, :, :volume.shape[2]]\n",
        "            result = temp\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _preprocess_volume(self, img_data):\n",
        "        \"\"\"\n",
        "        Preprocess fMRI volume.\n",
        "\n",
        "        Args:\n",
        "            img_data (np.ndarray): Input fMRI data\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Preprocessed volume\n",
        "        \"\"\"\n",
        "        # Handle 4D data\n",
        "        if len(img_data.shape) == 4:\n",
        "            img_data = np.mean(img_data, axis=-1)\n",
        "\n",
        "        # Convert to float32\n",
        "        img_data = img_data.astype(np.float32)\n",
        "\n",
        "        # Z-score normalization\n",
        "        mean = np.mean(img_data)\n",
        "        std = np.std(img_data)\n",
        "        img_data = (img_data - mean) / (std + 1e-8)\n",
        "\n",
        "        # Pad or crop to target size\n",
        "        img_data = self._pad_or_crop(img_data, (64, 64, 64))\n",
        "\n",
        "        return img_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.item()\n",
        "\n",
        "        file_path, stage, task = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Load the NIfTI file\n",
        "            img = nib.load(file_path)\n",
        "            img_data = img.get_fdata()\n",
        "\n",
        "            print(f\"Original shape for {file_path}: {img_data.shape}\")\n",
        "\n",
        "            # Preprocess\n",
        "            img_data = self._preprocess_volume(img_data)\n",
        "\n",
        "            # Add channel dimension\n",
        "            img_data = img_data[np.newaxis, ...].astype(np.float32)\n",
        "\n",
        "            # Convert to tensor\n",
        "            img_tensor = torch.from_numpy(img_data).float()\n",
        "\n",
        "            if self.transform:\n",
        "                img_tensor = self.transform(img_tensor)\n",
        "\n",
        "            return img_tensor, torch.tensor(stage, dtype=torch.long), task\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            # Return valid dummy data in case of error\n",
        "            return (\n",
        "                torch.zeros((1, 64, 64, 64), dtype=torch.float32),\n",
        "                torch.tensor(-1, dtype=torch.long),\n",
        "                \"error\"\n",
        "            )\n",
        "\n",
        "def visualize_sample(self, idx):\n",
        "    \"\"\"\n",
        "    Visualize a sample from all three anatomical perspectives.\n",
        "\n",
        "    Args:\n",
        "        idx (int): Index of the sample to visualize\n",
        "    \"\"\"\n",
        "    print(f\"\\nVisualizing sample {idx} from all perspectives:\")\n",
        "    for axis in range(3):\n",
        "        self.visualize_slice(idx, axis=axis)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path, stage, task = self.samples[idx]\n",
        "        try:\n",
        "            img = nib.load(file_path).get_fdata()\n",
        "\n",
        "            img = (img - img.mean()) / (img.std() + 1e-8)\n",
        "\n",
        "            current_shape = img.shape[:3]\n",
        "            target_shape = (64, 64, 64)\n",
        "\n",
        "            padded_img = np.zeros(target_shape)\n",
        "            for dim in range(3):\n",
        "                if current_shape[dim] > target_shape[dim]:\n",
        "                    start = (current_shape[dim] - target_shape[dim]) // 2\n",
        "                    if dim == 0:\n",
        "                        img = img[start:start+target_shape[dim], :, :]\n",
        "                    elif dim == 1:\n",
        "                        img = img[:, start:start+target_shape[dim], :]\n",
        "                    else:\n",
        "                        img = img[:, :, start:start+target_shape[dim]]\n",
        "                else:\n",
        "                    start = (target_shape[dim] - current_shape[dim]) // 2\n",
        "                    if dim == 0:\n",
        "                        padded_img[start:start+current_shape[dim], :, :] = img\n",
        "                    elif dim == 1:\n",
        "                        padded_img[:, start:start+current_shape[dim], :] = img\n",
        "                    else:\n",
        "                        padded_img[:, :, start:start+current_shape[dim]] = img\n",
        "                    img = padded_img\n",
        "\n",
        "            img = img[np.newaxis, ...]\n",
        "\n",
        "            img_tensor = torch.from_numpy(img).float()\n",
        "\n",
        "            if self.transform:\n",
        "                img_tensor = self.transform(img_tensor)\n",
        "\n",
        "            return img_tensor, stage, task\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            return torch.zeros((1, 64, 64, 64)), -1, \"error\""
      ],
      "metadata": {
        "id": "DBrLhcz2PbU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset(dataset, num_samples=3):\n",
        "    print(\"\\nTesting dataset access...\")\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        img, stage, task = dataset[i]\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"Image shape: {img.shape}\")\n",
        "        print(f\"Image stats - Min: {img.min():.4f}, Max: {img.max():.4f}, Mean: {img.mean():.4f}, Std: {img.std():.4f}\")\n",
        "        print(f\"Stage: {stage}\")\n",
        "        print(f\"Task: {task}\")\n",
        "\n",
        "        # Visualize middle slice\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img[0, :, :, 32], cmap='gray')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Sample {i} - Middle Slice\\nTask: {task}, Stage: {\"Early\" if stage == 0 else \"Late\"}')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "mbqCxnMqFyCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "uMUn-utXPdPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_improved(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    # Warmup parameters\n",
        "    warmup_epochs = 5\n",
        "    warmup_lr_init = 1e-6\n",
        "    base_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Warmup learning rate\n",
        "        if epoch < warmup_epochs:\n",
        "            lr = warmup_lr_init + (base_lr - warmup_lr_init) * epoch / warmup_epochs\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels, _ in train_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100.*train_correct/train_total:.2f}%',\n",
        "                'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc='Validation')\n",
        "            for inputs, labels, _ in val_bar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                val_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{100.*val_correct/val_total:.2f}%'\n",
        "                })\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}\\n')\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "GRkkBbc8PgOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "    plt.plot(history['val_acc'], label='Val Acc')\n",
        "    plt.title('Accuracy History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qVsRercIPkEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(true_labels, pred_labels):\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8hlMxj9JPlY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation and Model Training"
      ],
      "metadata": {
        "id": "8rDp9NTFPmlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Data augmentation for fMRI"
      ],
      "metadata": {
        "id": "ts0bXTVR6iAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fMRIAugmentation:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def gaussian_noise(self, x, std=0.01):\n",
        "        if torch.rand(1) < self.p:\n",
        "            return x + torch.randn_like(x) * std\n",
        "        return x\n",
        "\n",
        "    def random_flip(self, x):\n",
        "        if torch.rand(1) < self.p:\n",
        "            dims = torch.randint(1, 4, (1,)).item()\n",
        "            return torch.flip(x, [dims])\n",
        "        return x\n",
        "\n",
        "    def random_rotate(self, x):\n",
        "        if torch.rand(1) < self.p:\n",
        "            k = torch.randint(1, 4, (1,)).item()\n",
        "            dim1, dim2 = torch.randperm(3)[:2] + 1\n",
        "            return torch.rot90(x, k, [dim1, dim2])\n",
        "        return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.gaussian_noise(x)\n",
        "        x = self.random_flip(x)\n",
        "        x = self.random_rotate(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o5skmYW16kqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize dataset"
      ],
      "metadata": {
        "id": "U3STaMO0Pn7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing dataset...\")\n",
        "root_dir = '/content/drive/MyDrive/learnedSpectrum/fmri_data'\n",
        "augmentation = fMRIAugmentation(p=0.5)\n",
        "dataset = fMRIDataset(root_dir, transform=augmentation)\n",
        "test_dataset(dataset)"
      ],
      "metadata": {
        "id": "T8wkHvU1Pozu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create weighted sampler for balanced batches"
      ],
      "metadata": {
        "id": "LkCrTJ695hbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [sample[1] for sample in dataset.samples]\n",
        "labels = np.array(labels)\n",
        "unique_labels = np.unique(labels)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_labels,\n",
        "    y=labels\n",
        ")"
      ],
      "metadata": {
        "id": "mBZ4BHBU5iTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClass distribution:\")\n",
        "for label, weight in zip(unique_labels, class_weights):\n",
        "    count = np.sum(labels == label)\n",
        "    print(f\"Class {label}: {count} samples, weight: {weight:.4f}\")"
      ],
      "metadata": {
        "id": "wc-udJe17OJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_weights = [class_weights[label] for label in labels]\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")"
      ],
      "metadata": {
        "id": "NmCiKzf17PwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders"
      ],
      "metadata": {
        "id": "ka_Zuui3PugY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "3oxs28woPwDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "mCa2mJHYPxH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization and Training"
      ],
      "metadata": {
        "id": "22peC5vTPyRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device"
      ],
      "metadata": {
        "id": "WnNH7Pk8PzdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "WcDmoNTZP0g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize model, loss function and optimizer"
      ],
      "metadata": {
        "id": "9SxYX5LsP1xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_training():\n",
        "    # Initialize dataset\n",
        "    dataset = fMRIDataset(root_dir)\n",
        "\n",
        "    # Calculate class weights\n",
        "    labels = torch.tensor([sample[1] for sample in dataset.samples])\n",
        "    label_counts = torch.bincount(labels)\n",
        "    weights = len(labels) / (2 * label_counts.float())\n",
        "    class_weights = weights.float()  # Ensure float32\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedVisionTransformer3D(\n",
        "        img_size=64,\n",
        "        patch_size=8,\n",
        "        in_channels=1,\n",
        "        num_classes=2,\n",
        "        embed_dim=256,\n",
        "        depth=6,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4.,\n",
        "        qkv_bias=True,\n",
        "        drop_rate=0.2,\n",
        "        attn_drop_rate=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(\n",
        "        weight=class_weights.to(device)\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=True,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    return model, criterion, optimizer, scheduler, train_loader, val_loader"
      ],
      "metadata": {
        "id": "zEq4FFRPP3V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "QoETCTaEP9Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting up training components...\")\n",
        "model, criterion, optimizer, scheduler, train_loader, val_loader = setup_training()"
      ],
      "metadata": {
        "id": "LhnUGS1W8DUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "history = train_model_improved(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=50,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "tE19OqKU_FXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training history"
      ],
      "metadata": {
        "id": "kCZxG90zP_kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "Ekg7K5g-QAhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "z7ZEaedUQCX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load best model"
      ],
      "metadata": {
        "id": "JL80UsEjQDbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "MTU-Q5mwQHcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on test set"
      ],
      "metadata": {
        "id": "1katq7CnQI61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "all_preds = []\n",
        "all_labels = []"
      ],
      "metadata": {
        "id": "_MHeMXGMQKHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating model on test set...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels, _ in tqdm(test_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "uoV6N-ChQK_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate and print test metrics"
      ],
      "metadata": {
        "id": "-czK1j98QMX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 100. * test_correct / test_total\n",
        "print(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Early', 'Late']))"
      ],
      "metadata": {
        "id": "I_fQksfbQNau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot confusion matrix"
      ],
      "metadata": {
        "id": "zuRJzt5NQOjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConfusion Matrix:\")\n",
        "plot_confusion_matrix(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "U4Z8ig8vQPd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model and Results"
      ],
      "metadata": {
        "id": "tCIYGAUsQQqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save final model and results"
      ],
      "metadata": {
        "id": "tu8C7xRdQRsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'history': history,\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_predictions': all_preds,\n",
        "    'test_labels': all_labels\n",
        "}\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'results': results\n",
        "}, 'fmri_vit_model_results.pth')\n",
        "\n",
        "print(\"\\nModel and results saved to 'fmri_vit_model_results.pth'\")"
      ],
      "metadata": {
        "id": "sF0Kp8MTQTDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}