{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Learning Stage Classification with Vision Transformers\n",
    "\n",
    "This notebook demonstrates the use of Vision Transformers for classifying different stages of learning from fMRI data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/twarn/Repositories/learnedSpectrum\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (0.20.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: nibabel in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (5.3.2)\n",
      "Requirement already satisfied: nilearn in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (1.5.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (4.46.3)\n",
      "Requirement already satisfied: wandb in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (0.18.7)\n",
      "Requirement already satisfied: lru-dict in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: pywavelets in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: einops in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: timm in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (1.0.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (1.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from learnedSpectrum==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torch>=2.0->learnedSpectrum==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->learnedSpectrum==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from nibabel->learnedSpectrum==0.1.0) (6.4.5)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from nibabel->learnedSpectrum==0.1.0) (24.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from nilearn->learnedSpectrum==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from nilearn->learnedSpectrum==0.1.0) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from pandas->learnedSpectrum==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from pandas->learnedSpectrum==0.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from pandas->learnedSpectrum==0.1.0) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from requests->learnedSpectrum==0.1.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from requests->learnedSpectrum==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from requests->learnedSpectrum==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from requests->learnedSpectrum==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from scikit-learn->learnedSpectrum==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from timm->learnedSpectrum==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from timm->learnedSpectrum==0.1.0) (0.26.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from timm->learnedSpectrum==0.1.0) (0.4.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from torchvision->learnedSpectrum==0.1.0) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from tqdm->learnedSpectrum==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from transformers->learnedSpectrum==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from transformers->learnedSpectrum==0.1.0) (0.20.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from wandb->learnedSpectrum==0.1.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb->learnedSpectrum==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->learnedSpectrum==0.1.0) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from jinja2->torch>=2.0->learnedSpectrum==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\twarn\\repositories\\learnedspectrum\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->learnedSpectrum==0.1.0) (5.0.1)\n",
      "Building wheels for collected packages: learnedSpectrum\n",
      "  Building editable for learnedSpectrum (pyproject.toml): started\n",
      "  Building editable for learnedSpectrum (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for learnedSpectrum: filename=learnedSpectrum-0.1.0-0.editable-py3-none-any.whl size=7719 sha256=74979da240ef5436af38c61aaba046c3950fcfb2b351075cf949b4706c4448bb\n",
      "  Stored in directory: C:\\Users\\twarn\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-oglmvdmk\\wheels\\df\\e2\\01\\25b890ddbee843adacdabb258984995d70b3d4b4901a7c5b3a\n",
      "Successfully built learnedSpectrum\n",
      "Installing collected packages: learnedSpectrum\n",
      "  Attempting uninstall: learnedSpectrum\n",
      "    Found existing installation: learnedSpectrum 0.1.0\n",
      "    Uninstalling learnedSpectrum-0.1.0:\n",
      "      Successfully uninstalled learnedSpectrum-0.1.0\n",
      "Successfully installed learnedSpectrum-0.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Install package in editable mode if not already installed\n",
    "!pip install -e {project_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from learnedSpectrum.config import Config, DataConfig\n",
    "from learnedSpectrum.data import BIDSManager, NiftiLoader, DatasetManager, create_dataloaders\n",
    "from learnedSpectrum.train import VisionTransformerModel, train_one_epoch, evaluate, LabelSmoothingLoss, get_scheduler, load_best_model\n",
    "from learnedSpectrum.visualization import VisualizationManager\n",
    "from learnedSpectrum.utils import (\n",
    "    seed_everything,\n",
    "    get_optimizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    calculate_metrics,\n",
    "    verify_model_devices\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: tawarner (tawarner-usc). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING Path C:\\Users\\twarn\\Repositories\\learnedSpectrum\\wandb\\wandb\\ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\twarn\\AppData\\Local\\Temp\\wandb\\run-20241211_152854-z9so5398</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tawarner-usc/fmri-learning-stages/runs/z9so5398' target=\"_blank\">fearless-galaxy-350</a></strong> to <a href='https://wandb.ai/tawarner-usc/fmri-learning-stages' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tawarner-usc/fmri-learning-stages' target=\"_blank\">https://wandb.ai/tawarner-usc/fmri-learning-stages</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tawarner-usc/fmri-learning-stages/runs/z9so5398' target=\"_blank\">https://wandb.ai/tawarner-usc/fmri-learning-stages/runs/z9so5398</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize configurations\n",
    "config = Config()\n",
    "data_config = DataConfig()\n",
    "\n",
    "os.makedirs(config.CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# Set up visualization\n",
    "viz = VisualizationManager(save_dir=Path(config.ROOT) / \"visualizations\")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project='fmri-learning-stages',\n",
    "    config=vars(config),\n",
    "    dir=Path(config.ROOT) / \"wandb\"\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing 102 files from ds000002\n",
      "INFO:learnedSpectrum.data:All files already processed\n",
      "INFO:__main__:Processing 70 files from ds000011\n",
      "INFO:learnedSpectrum.data:All files already processed\n",
      "INFO:__main__:Processing 74 files from ds000017\n",
      "INFO:learnedSpectrum.data:All files already processed\n",
      "INFO:__main__:Processing 52 files from ds000052\n",
      "INFO:learnedSpectrum.data:All files already processed\n",
      "INFO:__main__:Preprocessing complete\n",
      "INFO:learnedSpectrum.data:Found 298 cached samples in C:\\Users\\twarn\\Repositories\\learnedSpectrum\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Initialize managers\n",
    "bids_manager = BIDSManager(data_config)\n",
    "nifti_loader = NiftiLoader(data_config, bids_manager)\n",
    "\n",
    "# Download and preprocess datasets\n",
    "for dataset_id in data_config.DATASET_URLS.keys():\n",
    "    try:\n",
    "        # Find dataset root\n",
    "        dataset_root = bids_manager._find_dataset_root(dataset_id)\n",
    "        \n",
    "        # Find all valid NIFTI files\n",
    "        nifti_files = []\n",
    "        for path in dataset_root.rglob(\"*bold.nii.gz\"):\n",
    "            if bids_manager.validate_nifti(path):\n",
    "                nifti_files.append(path)\n",
    "        \n",
    "        if not nifti_files:\n",
    "            logger.warning(f\"No valid NIFTI files found for {dataset_id}\")\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Processing {len(nifti_files)} files from {dataset_id}\")\n",
    "        \n",
    "        # Preprocess files\n",
    "        nifti_loader._parallel_preprocess(\n",
    "            paths=nifti_files,\n",
    "            max_workers=min(4, os.cpu_count() or 1)\n",
    "        )\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Dataset {dataset_id} not found, skipping\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {dataset_id}: {str(e)}\")\n",
    "        continue\n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"Preprocessing complete\")\n",
    "\n",
    "# Now initialize dataset manager\n",
    "dataset_manager = DatasetManager(config, data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preparing datasets...\n",
      "INFO:learnedSpectrum.data:Found 298 cached samples in C:\\Users\\twarn\\Repositories\\learnedSpectrum\\data\\processed\n",
      "INFO:learnedSpectrum.data:Label distribution:\n",
      "INFO:learnedSpectrum.data:Label 0: 109 samples\n",
      "INFO:learnedSpectrum.data:Label 1: 92 samples\n",
      "INFO:learnedSpectrum.data:Label 2: 71 samples\n",
      "INFO:learnedSpectrum.data:Label 3: 26 samples\n",
      "INFO:learnedSpectrum.data:Creating dataset with 208 paths and 208 labels\n",
      "INFO:learnedSpectrum.data:Maximum timepoints found: 237\n",
      "INFO:learnedSpectrum.data:Dataset initialized with 208 samples\n",
      "INFO:learnedSpectrum.data:Max timepoints: 237\n",
      "INFO:learnedSpectrum.data:Creating dataset with 45 paths and 45 labels\n",
      "INFO:learnedSpectrum.data:Maximum timepoints found: 237\n",
      "INFO:learnedSpectrum.data:Dataset initialized with 45 samples\n",
      "INFO:learnedSpectrum.data:Max timepoints: 237\n",
      "INFO:learnedSpectrum.data:Creating dataset with 45 paths and 45 labels\n",
      "INFO:learnedSpectrum.data:Maximum timepoints found: 237\n",
      "INFO:learnedSpectrum.data:Dataset initialized with 45 samples\n",
      "INFO:learnedSpectrum.data:Max timepoints: 237\n",
      "INFO:learnedSpectrum.data:\n",
      "Train set label distribution:\n",
      "INFO:learnedSpectrum.data:Label 0: 76 samples\n",
      "INFO:learnedSpectrum.data:Label 1: 64 samples\n",
      "INFO:learnedSpectrum.data:Label 2: 50 samples\n",
      "INFO:learnedSpectrum.data:Label 3: 18 samples\n",
      "INFO:learnedSpectrum.data:\n",
      "Val set label distribution:\n",
      "INFO:learnedSpectrum.data:Label 0: 16 samples\n",
      "INFO:learnedSpectrum.data:Label 1: 14 samples\n",
      "INFO:learnedSpectrum.data:Label 2: 11 samples\n",
      "INFO:learnedSpectrum.data:Label 3: 4 samples\n",
      "INFO:learnedSpectrum.data:\n",
      "Test set label distribution:\n",
      "INFO:learnedSpectrum.data:Label 0: 17 samples\n",
      "INFO:learnedSpectrum.data:Label 1: 14 samples\n",
      "INFO:learnedSpectrum.data:Label 2: 10 samples\n",
      "INFO:learnedSpectrum.data:Label 3: 4 samples\n",
      "INFO:learnedSpectrum.data:Loader sizes: train=13, val=3, test=3\n",
      "INFO:learnedSpectrum.data:Batch size: 16\n",
      "INFO:learnedSpectrum.data:Unique labels in training set: [0, 1, 2, 3]\n",
      "INFO:__main__:Dataset sizes: train=208, val=45, test=45\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "logger.info(\"Preparing datasets...\")\n",
    "dataset_manager = DatasetManager(config, data_config)\n",
    "train_ds, val_ds, test_ds = dataset_manager.prepare_datasets()\n",
    "\n",
    "# Create dataloaders with memory-efficient settings\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_ds, val_ds, test_ds, config\n",
    ")\n",
    "\n",
    "# Verify data was loaded\n",
    "logger.info(f\"Dataset sizes: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and visualize a sample\n",
    "sample_volume, sample_label = train_ds[90]\n",
    "viz.plot_brain_slice(\n",
    "    volume=sample_volume.numpy(),\n",
    "    time_idx=0,  # View first timepoint\n",
    "    title=f'Sample Brain Slice (Learning Stage: {sample_label})',\n",
    "    save_name='sample_slice'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twarn\\Repositories\\learnedSpectrum\\learnedSpectrum\\train.py:175: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  pe[:, 0::2] = torch.sin(pos * omega.T)\n",
      "INFO:learnedSpectrum.utils:model on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = VisionTransformerModel(config)\n",
    "verify_model_devices(model)\n",
    "\n",
    "# Setup training components\n",
    "optimizer = get_optimizer(model, config)\n",
    "\n",
    "# Ensure optimizer params are in FP32\n",
    "for param_group in optimizer.param_groups:\n",
    "    for param in param_group['params']:\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data.float()\n",
    "            \n",
    "scaler = torch.amp.GradScaler('cuda', enabled=config.USE_AMP)\n",
    "\n",
    "criterion = LabelSmoothingLoss(classes=9, smoothing=0.1)  # 9 classes from your dataset\n",
    "\n",
    "# Update the scheduler initialization\n",
    "num_training_steps = config.NUM_EPOCHS * len(train_loader)\n",
    "num_warmup_steps = num_training_steps // 10  # 10% warmup\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=num_warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader lens: train=13, val=3\n",
      "Unique labels in dataset: [0, 1, 2, 3]\n",
      "Number of classes: 4\n",
      "batch peek: torch.Size([16, 64, 64, 30, 237])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Epoch 1/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_0.pth\n",
      "INFO:__main__:Epoch 1 - Train Loss: 1.6950, Train Acc: 0.2356, Val Loss: 1.6341, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 2/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_1.pth\n",
      "INFO:__main__:Epoch 2 - Train Loss: 1.5688, Train Acc: 0.2019, Val Loss: 1.5254, Val Acc: 0.2667\n",
      "INFO:__main__:\n",
      "Epoch 3/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_2.pth\n",
      "INFO:__main__:Epoch 3 - Train Loss: 1.4699, Train Acc: 0.2788, Val Loss: 1.4692, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 4/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_3.pth\n",
      "INFO:__main__:Epoch 4 - Train Loss: 1.4321, Train Acc: 0.2885, Val Loss: 1.4283, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 5/50\n",
      "INFO:__main__:Epoch 5 - Train Loss: 1.4184, Train Acc: 0.4087, Val Loss: 1.5039, Val Acc: 0.1778\n",
      "INFO:__main__:\n",
      "Epoch 6/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_5.pth\n",
      "INFO:__main__:Epoch 6 - Train Loss: 1.3853, Train Acc: 0.3029, Val Loss: 1.3545, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 7/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_6.pth\n",
      "INFO:__main__:Epoch 7 - Train Loss: 1.4298, Train Acc: 0.3077, Val Loss: 1.3329, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 8/50\n",
      "INFO:__main__:Epoch 8 - Train Loss: 1.3755, Train Acc: 0.4038, Val Loss: 1.4335, Val Acc: 0.2444\n",
      "INFO:__main__:\n",
      "Epoch 9/50\n",
      "INFO:__main__:Epoch 9 - Train Loss: 1.3967, Train Acc: 0.3558, Val Loss: 1.4684, Val Acc: 0.2222\n",
      "INFO:__main__:\n",
      "Epoch 10/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_9.pth\n",
      "INFO:__main__:Epoch 10 - Train Loss: 1.4656, Train Acc: 0.3606, Val Loss: 1.3305, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 11/50\n",
      "INFO:__main__:Epoch 11 - Train Loss: 1.3682, Train Acc: 0.2788, Val Loss: 1.3470, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 12/50\n",
      "INFO:__main__:Epoch 12 - Train Loss: 1.3344, Train Acc: 0.4135, Val Loss: 1.4056, Val Acc: 0.2222\n",
      "INFO:__main__:\n",
      "Epoch 13/50\n",
      "INFO:__main__:Epoch 13 - Train Loss: 1.3585, Train Acc: 0.2356, Val Loss: 1.3422, Val Acc: 0.1778\n",
      "INFO:__main__:\n",
      "Epoch 14/50\n",
      "INFO:__main__:Epoch 14 - Train Loss: 1.3300, Train Acc: 0.4231, Val Loss: 1.4137, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 15/50\n",
      "INFO:__main__:Epoch 15 - Train Loss: 1.2826, Train Acc: 0.3654, Val Loss: 1.4126, Val Acc: 0.2222\n",
      "INFO:__main__:\n",
      "Epoch 16/50\n",
      "INFO:__main__:Epoch 16 - Train Loss: 1.2328, Train Acc: 0.4135, Val Loss: 1.3692, Val Acc: 0.2889\n",
      "INFO:__main__:\n",
      "Epoch 17/50\n",
      "INFO:__main__:Epoch 17 - Train Loss: 1.1999, Train Acc: 0.4183, Val Loss: 1.3737, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 18/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_17.pth\n",
      "INFO:__main__:Epoch 18 - Train Loss: 1.1718, Train Acc: 0.4712, Val Loss: 1.3181, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 19/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_18.pth\n",
      "INFO:__main__:Epoch 19 - Train Loss: 1.2202, Train Acc: 0.3413, Val Loss: 1.2792, Val Acc: 0.2667\n",
      "INFO:__main__:\n",
      "Epoch 20/50\n",
      "INFO:__main__:Epoch 20 - Train Loss: 1.1411, Train Acc: 0.5288, Val Loss: 1.4254, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 21/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_20.pth\n",
      "INFO:__main__:Epoch 21 - Train Loss: 1.2800, Train Acc: 0.3558, Val Loss: 1.2220, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 22/50\n",
      "INFO:__main__:Epoch 22 - Train Loss: 1.0576, Train Acc: 0.4519, Val Loss: 1.2954, Val Acc: 0.2889\n",
      "INFO:__main__:\n",
      "Epoch 23/50\n",
      "INFO:__main__:Epoch 23 - Train Loss: 1.0833, Train Acc: 0.4952, Val Loss: 1.2745, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 24/50\n",
      "INFO:__main__:Epoch 24 - Train Loss: 1.0535, Train Acc: 0.4471, Val Loss: 1.2274, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 25/50\n",
      "INFO:__main__:Epoch 25 - Train Loss: 1.1065, Train Acc: 0.4087, Val Loss: 1.2855, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 26/50\n",
      "INFO:__main__:Epoch 26 - Train Loss: 1.1573, Train Acc: 0.4615, Val Loss: 1.2261, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 27/50\n",
      "INFO:__main__:Epoch 27 - Train Loss: 1.0260, Train Acc: 0.5481, Val Loss: 1.3838, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 28/50\n",
      "INFO:__main__:Epoch 28 - Train Loss: 1.0823, Train Acc: 0.4135, Val Loss: 1.2396, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 29/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_28.pth\n",
      "INFO:__main__:Epoch 29 - Train Loss: 1.1078, Train Acc: 0.4423, Val Loss: 1.2105, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 30/50\n",
      "INFO:__main__:Epoch 30 - Train Loss: 0.9773, Train Acc: 0.4904, Val Loss: 1.2549, Val Acc: 0.4000\n",
      "INFO:__main__:\n",
      "Epoch 31/50\n",
      "INFO:__main__:Epoch 31 - Train Loss: 1.1279, Train Acc: 0.4327, Val Loss: 1.2276, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 32/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_31.pth\n",
      "INFO:__main__:Epoch 32 - Train Loss: 0.9683, Train Acc: 0.5240, Val Loss: 1.1724, Val Acc: 0.3778\n",
      "INFO:__main__:\n",
      "Epoch 33/50\n",
      "INFO:__main__:Epoch 33 - Train Loss: 1.0030, Train Acc: 0.5433, Val Loss: 1.3237, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 34/50\n",
      "INFO:__main__:Epoch 34 - Train Loss: 0.9422, Train Acc: 0.5337, Val Loss: 1.2225, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 35/50\n",
      "INFO:__main__:Epoch 35 - Train Loss: 1.0281, Train Acc: 0.4904, Val Loss: 1.3209, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 36/50\n",
      "INFO:__main__:Epoch 36 - Train Loss: 1.2440, Train Acc: 0.4567, Val Loss: 1.4765, Val Acc: 0.2889\n",
      "INFO:__main__:\n",
      "Epoch 37/50\n",
      "INFO:__main__:Epoch 37 - Train Loss: 1.0261, Train Acc: 0.5433, Val Loss: 1.3515, Val Acc: 0.4000\n",
      "INFO:__main__:\n",
      "Epoch 38/50\n",
      "INFO:__main__:Epoch 38 - Train Loss: 1.1162, Train Acc: 0.4952, Val Loss: 1.3606, Val Acc: 0.2667\n",
      "INFO:__main__:\n",
      "Epoch 39/50\n",
      "INFO:__main__:Epoch 39 - Train Loss: 1.0229, Train Acc: 0.5096, Val Loss: 1.2713, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 40/50\n",
      "INFO:__main__:Epoch 40 - Train Loss: 1.0191, Train Acc: 0.5769, Val Loss: 1.2966, Val Acc: 0.3778\n",
      "INFO:__main__:\n",
      "Epoch 41/50\n",
      "INFO:__main__:Epoch 41 - Train Loss: 1.0739, Train Acc: 0.4760, Val Loss: 1.3096, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 42/50\n",
      "INFO:__main__:Epoch 42 - Train Loss: 0.9708, Train Acc: 0.5337, Val Loss: 1.3241, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 43/50\n",
      "INFO:__main__:Epoch 43 - Train Loss: 1.0133, Train Acc: 0.4231, Val Loss: 1.2019, Val Acc: 0.3333\n",
      "INFO:__main__:\n",
      "Epoch 44/50\n",
      "INFO:learnedSpectrum.utils:checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_43.pth\n",
      "INFO:__main__:Epoch 44 - Train Loss: 1.0796, Train Acc: 0.4231, Val Loss: 1.1605, Val Acc: 0.4222\n",
      "INFO:__main__:\n",
      "Epoch 45/50\n",
      "INFO:__main__:Epoch 45 - Train Loss: 1.1418, Train Acc: 0.4567, Val Loss: 1.4837, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 46/50\n",
      "INFO:__main__:Epoch 46 - Train Loss: 0.9805, Train Acc: 0.5817, Val Loss: 1.2273, Val Acc: 0.3556\n",
      "INFO:__main__:\n",
      "Epoch 47/50\n",
      "INFO:__main__:Epoch 47 - Train Loss: 1.1792, Train Acc: 0.4327, Val Loss: 1.3727, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 48/50\n",
      "INFO:__main__:Epoch 48 - Train Loss: 1.1557, Train Acc: 0.4567, Val Loss: 1.4111, Val Acc: 0.3111\n",
      "INFO:__main__:\n",
      "Epoch 49/50\n",
      "INFO:__main__:Epoch 49 - Train Loss: 0.9620, Train Acc: 0.5529, Val Loss: 1.2061, Val Acc: 0.3778\n",
      "INFO:__main__:\n",
      "Epoch 50/50\n",
      "INFO:__main__:Epoch 50 - Train Loss: 1.0892, Train Acc: 0.5288, Val Loss: 1.4308, Val Acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"loader lens: train={len(train_loader)}, val={len(val_loader)}\")\n",
    "\n",
    "all_labels = []\n",
    "for _, labels in train_loader:\n",
    "    all_labels.extend(labels.tolist())\n",
    "unique_labels = sorted(set(all_labels))\n",
    "print(f\"Unique labels in dataset: {unique_labels}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "# Safe batch peek without timeout\n",
    "try:\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"batch peek: {batch[0].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Batch peek failed (this is ok): {str(e)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    logger.info(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, scaler, config)\n",
    "    train_loss, train_metrics = evaluate(model, train_loader, config)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = evaluate(model, val_loader, config)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Plot training progress\n",
    "    viz.plot_training_history(history, save_name=f'training_history_epoch_{epoch}')\n",
    "    \n",
    "    # Log to wandb\n",
    "    viz.log_to_wandb({\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'train_metrics': train_metrics,\n",
    "        'val_metrics': val_metrics,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr']\n",
    "    }, epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, val_loss, config,\n",
    "            filename=f\"best_model_epoch_{epoch}.pth\"\n",
    "        )\n",
    "        \n",
    "    logger.info(\n",
    "        f\"Epoch {epoch + 1} - \"\n",
    "        f\"Train Loss: {train_loss:.4f}, \"\n",
    "        f\"Train Acc: {train_metrics['accuracy']:.4f}, \"\n",
    "        f\"Val Loss: {val_loss:.4f}, \"\n",
    "        f\"Val Acc: {val_metrics['accuracy']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded checkpoint: C:\\Users\\twarn\\Repositories\\learnedSpectrum\\notebooks\\models\\best_model_epoch_43.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics\n",
      "========================\n",
      "Overall Accuracy: 0.356\n",
      "Balanced Accuracy: 0.428\n",
      "Macro F1: 0.407\n",
      "Cohen's Kappa: 0.093\n",
      "Mean Loss: 1.082 (±0.257)\n",
      "\n",
      "Per-class Performance\n",
      "====================\n",
      "\n",
      "Early:\n",
      "Precision: 0.286\n",
      "Recall: 0.235\n",
      "F1: 0.258\n",
      "ROC AUC: 0.368\n",
      "Support: 17\n",
      "\n",
      "Middle:\n",
      "Precision: 0.353\n",
      "Recall: 0.429\n",
      "F1: 0.387\n",
      "ROC AUC: 0.555\n",
      "Support: 14\n",
      "\n",
      "Late:\n",
      "Precision: 0.333\n",
      "Recall: 0.300\n",
      "F1: 0.316\n",
      "ROC AUC: 0.740\n",
      "Support: 10\n",
      "\n",
      "Mastery:\n",
      "Precision: 0.600\n",
      "Recall: 0.750\n",
      "F1: 0.667\n",
      "ROC AUC: 0.945\n",
      "Support: 4\n",
      "\n",
      "Confusion Matrix (Normalized)\n",
      "============================\n",
      "[[0.23529412 0.47058824 0.29411765 0.        ]\n",
      " [0.35714286 0.42857143 0.07142857 0.14285714]\n",
      " [0.5        0.2        0.3        0.        ]\n",
      " [0.         0.25       0.         0.75      ]]\n",
      "\n",
      "Reliability Metrics\n",
      "==================\n",
      "Mean Confidence: 0.437\n",
      "Overconfidence: 0.088\n",
      "Expected Calibration Error: 0.491\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.serialization.add_safe_globals(['Config'])\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, cohen_kappa_score\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load best model checkpoint\n",
    "checkpoints = list(Path(config.CKPT_DIR).glob(\"best_model_epoch_*.pth\"))\n",
    "latest = max(checkpoints, key=lambda x: int(str(x).split(\"_\")[-1].split(\".\")[0]))\n",
    "model, _, _ = load_checkpoint(model, None, latest, weights_only=False)\n",
    "logger.info(f\"Loaded checkpoint: {latest}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "all_losses = []\n",
    "\n",
    "# Evaluation with mixed precision\n",
    "with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels, reduction='none')\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_losses.extend(loss.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "all_losses = np.array(all_losses)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "class_names = ['Early', 'Middle', 'Late', 'Mastery']\n",
    "metrics = {\n",
    "    'overall': {\n",
    "        'accuracy': (all_preds == all_labels).mean(),\n",
    "        'balanced_accuracy': balanced_accuracy_score(all_labels, all_preds),\n",
    "        'macro_f1': f1_score(all_labels, all_preds, average='macro'),\n",
    "        'weighted_f1': f1_score(all_labels, all_preds, average='weighted'),\n",
    "        'cohen_kappa': cohen_kappa_score(all_labels, all_preds),\n",
    "        'mean_loss': np.mean(all_losses),\n",
    "        'std_loss': np.std(all_losses)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Per-class metrics\n",
    "for i, class_name in enumerate(class_names):\n",
    "    metrics[class_name] = {\n",
    "        'precision': precision_score(all_labels == i, all_preds == i),\n",
    "        'recall': recall_score(all_labels == i, all_preds == i),\n",
    "        'f1': f1_score(all_labels == i, all_preds == i),\n",
    "        'support': np.sum(all_labels == i),\n",
    "        'roc_auc': roc_auc_score(all_labels == i, all_probs[:, i])\n",
    "    }\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "norm_conf_matrix = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "\n",
    "# Print results in paper-ready format\n",
    "print(\"\\nModel Performance Metrics\")\n",
    "print(\"========================\")\n",
    "print(f\"Overall Accuracy: {metrics['overall']['accuracy']:.3f}\")\n",
    "print(f\"Balanced Accuracy: {metrics['overall']['balanced_accuracy']:.3f}\")\n",
    "print(f\"Macro F1: {metrics['overall']['macro_f1']:.3f}\")\n",
    "print(f\"Cohen's Kappa: {metrics['overall']['cohen_kappa']:.3f}\")\n",
    "print(f\"Mean Loss: {metrics['overall']['mean_loss']:.3f} (±{metrics['overall']['std_loss']:.3f})\")\n",
    "\n",
    "print(\"\\nPer-class Performance\")\n",
    "print(\"====================\")\n",
    "for class_name in class_names:\n",
    "    m = metrics[class_name]\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"Precision: {m['precision']:.3f}\")\n",
    "    print(f\"Recall: {m['recall']:.3f}\")\n",
    "    print(f\"F1: {m['f1']:.3f}\")\n",
    "    print(f\"ROC AUC: {m['roc_auc']:.3f}\")\n",
    "    print(f\"Support: {m['support']}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (Normalized)\")\n",
    "print(\"============================\")\n",
    "print(norm_conf_matrix)\n",
    "\n",
    "# Calculate reliability metrics\n",
    "correct_probs = np.array([all_probs[i, all_labels[i]] for i in range(len(all_labels))])\n",
    "confidence = np.max(all_probs, axis=1)\n",
    "metrics['reliability'] = {\n",
    "    'mean_confidence': np.mean(confidence),\n",
    "    'overconfidence': np.mean(confidence - correct_probs),\n",
    "    'expected_calibration_error': np.mean(np.abs(confidence - (all_preds == all_labels)))\n",
    "}\n",
    "\n",
    "print(\"\\nReliability Metrics\")\n",
    "print(\"==================\")\n",
    "print(f\"Mean Confidence: {metrics['reliability']['mean_confidence']:.3f}\")\n",
    "print(f\"Overconfidence: {metrics['reliability']['overconfidence']:.3f}\")\n",
    "print(f\"Expected Calibration Error: {metrics['reliability']['expected_calibration_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table for Paper:\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{lcccc}\n",
      "\\toprule\n",
      "Class & Precision & Recall & F1 & Support \\\\\n",
      "\\midrule\n",
      "Early & 0.286 & 0.235 & 0.258 & 17 \\\\\n",
      "Middle & 0.353 & 0.429 & 0.387 & 14 \\\\\n",
      "Late & 0.333 & 0.300 & 0.316 & 10 \\\\\n",
      "Mastery & 0.600 & 0.750 & 0.667 & 4 \\\\\n",
      "\\midrule\n",
      "Overall & 0.407 & 0.428 & 0.347 & 45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Model Performance Metrics}\n",
      "\\label{tab:model_performance}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from pathlib import Path\n",
    "\n",
    "def create_publication_plots(metrics, all_preds, all_labels, all_probs, class_names, save_dir='figures/'):\n",
    "    # Set publication style\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (10, 8),\n",
    "        'figure.dpi': 300,\n",
    "        'font.family': 'Arial',\n",
    "        'font.size': 10,\n",
    "        'axes.linewidth': 0.5,\n",
    "        'axes.spines.top': False,\n",
    "        'axes.spines.right': False,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Figure 1: Overall Performance Summary\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = plt.GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # 1a: Confusion Matrix\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "    sns.heatmap(cm, annot=True, fmt='.2%', cmap='YlOrRd', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=ax1)\n",
    "    ax1.set_title('A. Normalized Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('True')\n",
    "\n",
    "    # 1b: ROC Curves\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for i, cls in enumerate(class_names):\n",
    "        y_true = (all_labels == i).astype(int)\n",
    "        y_score = all_probs[:, i]\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, label=f'{cls} (AUC={roc_auc:.2f})')\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax2.set_title('B. ROC Curves')\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # 1c: Performance Metrics\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    metric_names = ['Precision', 'Recall', 'F1']\n",
    "    class_metrics = np.array([[metrics[cls][m.lower()] for m in metric_names] \n",
    "                            for cls in class_names])\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    multiplier = 0\n",
    "    \n",
    "    for i, metric in enumerate(metric_names):\n",
    "        offset = width * multiplier\n",
    "        ax3.bar(x + offset, class_metrics[:, i], width, label=metric)\n",
    "        multiplier += 1\n",
    "\n",
    "    ax3.set_xticks(x + width, class_names)\n",
    "    ax3.set_title('C. Per-class Performance')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.grid(axis='y')\n",
    "\n",
    "    # 1d: Confidence Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    for i, cls in enumerate(class_names):\n",
    "        mask = all_labels == i\n",
    "        if mask.any():\n",
    "            sns.kdeplot(all_probs[mask, i], label=f'{cls}',\n",
    "                       ax=ax4)\n",
    "    \n",
    "    ax4.axvline(0.5, color='k', linestyle='--', alpha=0.3)\n",
    "    ax4.set_title('D. Prediction Confidence Distribution')\n",
    "    ax4.set_xlabel('Model Confidence')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.suptitle('Model Performance Analysis', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'performance_summary.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 2: Calibration Analysis\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    confidence = np.max(all_probs, axis=1)\n",
    "    correct = (all_preds == all_labels)\n",
    "    \n",
    "    # Create reliability diagram\n",
    "    n_bins = 10\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (confidence >= bins[i]) & (confidence < bins[i + 1])\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_accuracies.append(np.mean(correct[mask]))\n",
    "            bin_confidences.append(np.mean(confidence[mask]))\n",
    "            bin_counts.append(np.sum(mask))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "    plt.plot(bin_confidences, bin_accuracies, 'o-', label='Model Calibration')\n",
    "    \n",
    "    plt.title('Reliability Diagram')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_dir / 'calibration_analysis.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print LaTeX table for paper\n",
    "    print(\"\\nLaTeX Table for Paper:\")\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{lcccc}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\"Class & Precision & Recall & F1 & Support \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    for cls in class_names:\n",
    "        m = metrics[cls]\n",
    "        print(f\"{cls} & {m['precision']:.3f} & {m['recall']:.3f} & \"\n",
    "              f\"{m['f1']:.3f} & {int(m['support'])} \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\midrule\")\n",
    "    print(f\"Overall & {metrics['overall']['macro_f1']:.3f} & \"\n",
    "          f\"{metrics['overall']['balanced_accuracy']:.3f} & \"\n",
    "          f\"{metrics['overall']['weighted_f1']:.3f} & \"\n",
    "          f\"{sum([metrics[cls]['support'] for cls in class_names])} \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Model Performance Metrics}\")\n",
    "    print(\"\\\\label{tab:model_performance}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "\n",
    "create_publication_plots(metrics, all_preds, all_labels, all_probs, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
